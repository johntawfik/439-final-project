{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pprint import pprint\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('ratings.csv')\n",
    "movies = pd.read_csv('movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     userId  movieId  rating  timestamp\n",
      "55        1     1031     5.0  964982653\n",
      "230       1     4006     4.0  964982903\n",
      "69        1     1197     5.0  964981872\n",
      "168       1     2596     5.0  964981144\n",
      "109       1     1777     4.0  964981230\n"
     ]
    }
   ],
   "source": [
    "def train_test_split_user(ratings, test_size=0.2):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "\n",
    "    # Get all unique users\n",
    "    for user_id in ratings['userId'].unique():\n",
    "        # Filter the dataset to include only rows corresponding to curr user\n",
    "        user_data = ratings[ratings['userId'] == user_id]\n",
    "        # Check if user has rated more than 5 movies to meaningfully split data into train and test\n",
    "        # Ex. user with 10 ratings -> 8 train, 2 test, but user with 3 ratings -> 2 train, 1 test\n",
    "            # In the latter case, we would not have enough data to train the model\n",
    "        # If user has rated more than 5 movies, split the data into train and test\n",
    "        # Else, include all data in train\n",
    "        if len(user_data) >= 5:\n",
    "            train_data, test_data = train_test_split(user_data, test_size=test_size, random_state=42)\n",
    "            train_list.append(train_data)\n",
    "            test_list.append(test_data)\n",
    "        else:\n",
    "            train_list.append(user_data)\n",
    "    \n",
    "    # Combine training and testing data for all users into train and test\n",
    "    train = pd.concat(train_list)\n",
    "    test = pd.concat(test_list)\n",
    "    return train, test\n",
    "\n",
    "train_data, test_data = train_test_split_user(ratings)\n",
    "print(train_data.head())\n",
    "# # We don't need timestamp column\n",
    "# train_data = train_data.drop(columns=['timestamp'])\n",
    "# test_data = test_data.drop(columns=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.5), np.float64(1.0), np.float64(1.5), np.float64(2.0), np.float64(2.5), np.float64(3.0), np.float64(3.5), np.float64(4.0), np.float64(4.5), np.float64(5.0)]\n"
     ]
    }
   ],
   "source": [
    "all_user_ids = pd.concat([train_data['userId'], test_data['userId']]).unique()\n",
    "all_movie_ids = pd.concat([train_data['movieId'], test_data['movieId']]).unique()\n",
    "\n",
    "# Create mappings from IDs to indices\n",
    "user_id_mapping = {id: idx for idx, id in enumerate(all_user_ids)}\n",
    "movie_id_mapping = {id: idx for idx, id in enumerate(all_movie_ids)}\n",
    "id_movie_mapping = {idx: id for idx, id in enumerate(all_movie_ids)}\n",
    "\n",
    "# Apply mappings to the datasets\n",
    "train_data['user_index'] = train_data['userId'].map(user_id_mapping)\n",
    "train_data['movie_index'] = train_data['movieId'].map(movie_id_mapping)\n",
    "test_data['user_index'] = test_data['userId'].map(user_id_mapping)\n",
    "test_data['movie_index'] = test_data['movieId'].map(movie_id_mapping)\n",
    "\n",
    "# Encode the ratings as class indices\n",
    "rating_values = sorted(train_data['rating'].unique())\n",
    "print(rating_values)\n",
    "\n",
    "X_train_array = [train_data['user_index'].values, train_data['movie_index'].values]\n",
    "y_train = train_data['rating'].values\n",
    "X_test_array = [test_data['user_index'].values, test_data['movie_index'].values]\n",
    "y_test = test_data['rating'].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_class = label_encoder.fit_transform(y_train)\n",
    "y_test_class = label_encoder.fit_transform(y_test)\n",
    "\n",
    "# y_train_one_hot = torch.nn.functional.one_hot(torch.tensor(y_train_class), num_classes=len(rating_values)).float()\n",
    "# y_test_one_hot = torch.nn.functional.one_hot(torch.tensor(y_test_class), num_classes=len(rating_values)).float()\n",
    "\n",
    "_, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "val_data['user_index'] = val_data['userId'].map(user_id_mapping)\n",
    "val_data['movie_index'] = val_data['movieId'].map(movie_id_mapping)\n",
    "\n",
    "X_val_array = [val_data['user_index'].values, val_data['movie_index'].values]\n",
    "y_val = val_data['rating'].values\n",
    "y_val_class = label_encoder.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_normalize(ratings):\n",
    "    # Group by userId\n",
    "    normalized_ratings = ratings.groupby('userId')['rating'].transform(lambda x: quantile_scale(x))\n",
    "    ratings['rating'] = normalized_ratings\n",
    "    return ratings\n",
    "\n",
    "def quantile_scale(ratings):\n",
    "    # Sort ratings and assign ranks\n",
    "    sorted_ratings = np.sort(ratings)\n",
    "    ranks = np.argsort(np.argsort(ratings))  # Get ranks of the original ratings\n",
    "    # Assign the sorted values to original ranks (quantile transformation)\n",
    "    return np.interp(ratings, sorted_ratings, sorted_ratings)\n",
    "# abc = quantile_normalize(y_train_class)\n",
    "# print(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "4.0    21401\n",
      "3.0    16074\n",
      "5.0    10543\n",
      "3.5    10482\n",
      "4.5     6798\n",
      "2.0     5959\n",
      "2.5     4411\n",
      "1.0     2249\n",
      "1.5     1439\n",
      "0.5     1063\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print(X_val_array)\n",
    "# print(y_val)\n",
    "print(train_data['rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 128  # Embedding size\n",
    "dropout_rate = 0.2  # Dropout rate for regularization\n",
    "n_movies = len(set(train_data['movieId']))\n",
    "n_users = len(set(train_data['userId']))\n",
    "\n",
    "# User input and embedding\n",
    "user = tf.keras.layers.Input(shape=(1,))\n",
    "u = tf.keras.layers.Embedding(\n",
    "    n_users, K,\n",
    "    embeddings_initializer='he_normal',\n",
    "    embeddings_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    ")(user)\n",
    "u = tf.keras.layers.Reshape((K,))(u)\n",
    "\n",
    "# Apply a non-linear transformation to user embedding\n",
    "u_transformed = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal')(u)\n",
    "\n",
    "# Movie input and embedding\n",
    "movie = tf.keras.layers.Input(shape=(1,))\n",
    "m = tf.keras.layers.Embedding(\n",
    "    n_movies, K,\n",
    "    embeddings_initializer='he_normal',\n",
    "    embeddings_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    ")(movie)\n",
    "m = tf.keras.layers.Reshape((K,))(m)\n",
    "\n",
    "# Apply a non-linear transformation to movie embedding\n",
    "m_transformed = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal')(m)\n",
    "\n",
    "# Matrix factorization: Dot product between original embeddings\n",
    "mf_interaction = tf.keras.layers.Dot(axes=1)([u, m])\n",
    "\n",
    "# Concatenate transformed embeddings and MF interaction\n",
    "x = tf.keras.layers.Concatenate()([u_transformed, m_transformed, mf_interaction])\n",
    "\n",
    "# Fully connected layers\n",
    "# x = tf.keras.layers.Dense(256, activation='relu', kernel_initializer='he_normal')(x)\n",
    "# x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal')(x)\n",
    "x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n",
    "x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "# Output layer\n",
    "x = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "# Model definition\n",
    "model = tf.keras.models.Model(inputs=[user, movie], outputs=x)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), \n",
    "              loss='mean_squared_error', \n",
    "              metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vian Miranda\\.conda\\envs\\data_sci_439\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_4']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 0.0019 - mae: 0.1885 - val_loss: 0.0356 - val_mae: 0.1498 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 3.9651e-04 - mae: 0.1518 - val_loss: 0.0340 - val_mae: 0.1445 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 3.7702e-04 - mae: 0.1484 - val_loss: 0.0330 - val_mae: 0.1414 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 3.5841e-04 - mae: 0.1456 - val_loss: 0.0315 - val_mae: 0.1370 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 3.4173e-04 - mae: 0.1428 - val_loss: 0.0319 - val_mae: 0.1393 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 3.2994e-04 - mae: 0.1416 - val_loss: 0.0311 - val_mae: 0.1368 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 3.1866e-04 - mae: 0.1395 - val_loss: 0.0308 - val_mae: 0.1372 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 3.1027e-04 - mae: 0.1399 - val_loss: 0.0304 - val_mae: 0.1339 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 3.1700e-04 - mae: 0.1396 - val_loss: 0.0306 - val_mae: 0.1342 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 3.0978e-04 - mae: 0.1385 - val_loss: 0.0312 - val_mae: 0.1387 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m628/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.0309e-04 - mae: 0.1382\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 3.0313e-04 - mae: 0.1382 - val_loss: 0.0304 - val_mae: 0.1343 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.9285e-04 - mae: 0.1371 - val_loss: 0.0297 - val_mae: 0.1347 - learning_rate: 7.5000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.7539e-04 - mae: 0.1366 - val_loss: 0.0299 - val_mae: 0.1358 - learning_rate: 7.5000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.8347e-04 - mae: 0.1360 - val_loss: 0.0291 - val_mae: 0.1315 - learning_rate: 7.5000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.7723e-04 - mae: 0.1348 - val_loss: 0.0298 - val_mae: 0.1319 - learning_rate: 7.5000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 2.7749e-04 - mae: 0.1354 - val_loss: 0.0302 - val_mae: 0.1339 - learning_rate: 7.5000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.7807e-04 - mae: 0.1350\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.7808e-04 - mae: 0.1350 - val_loss: 0.0295 - val_mae: 0.1320 - learning_rate: 7.5000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.6617e-04 - mae: 0.1334 - val_loss: 0.0291 - val_mae: 0.1309 - learning_rate: 5.6250e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 2.5625e-04 - mae: 0.1336 - val_loss: 0.0291 - val_mae: 0.1307 - learning_rate: 5.6250e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 2.5626e-04 - mae: 0.1332 - val_loss: 0.0288 - val_mae: 0.1309 - learning_rate: 5.6250e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.5944e-04 - mae: 0.1329 - val_loss: 0.0286 - val_mae: 0.1314 - learning_rate: 5.6250e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 2.5253e-04 - mae: 0.1325 - val_loss: 0.0289 - val_mae: 0.1310 - learning_rate: 5.6250e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.5762e-04 - mae: 0.1333 - val_loss: 0.0292 - val_mae: 0.1315 - learning_rate: 5.6250e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m625/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.5730e-04 - mae: 0.1330\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.5735e-04 - mae: 0.1330 - val_loss: 0.0290 - val_mae: 0.1321 - learning_rate: 5.6250e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.4679e-04 - mae: 0.1313 - val_loss: 0.0287 - val_mae: 0.1323 - learning_rate: 4.2187e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.3902e-04 - mae: 0.1320 - val_loss: 0.0286 - val_mae: 0.1287 - learning_rate: 4.2187e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 2.4254e-04 - mae: 0.1312 - val_loss: 0.0281 - val_mae: 0.1292 - learning_rate: 4.2187e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 2.4257e-04 - mae: 0.1316 - val_loss: 0.0279 - val_mae: 0.1289 - learning_rate: 4.2187e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 2.4174e-04 - mae: 0.1313 - val_loss: 0.0281 - val_mae: 0.1284 - learning_rate: 4.2187e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 2.4202e-04 - mae: 0.1314 - val_loss: 0.0282 - val_mae: 0.1281 - learning_rate: 4.2187e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m625/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.4298e-04 - mae: 0.1310\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.4304e-04 - mae: 0.1310 - val_loss: 0.0280 - val_mae: 0.1276 - learning_rate: 4.2187e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.3368e-04 - mae: 0.1304 - val_loss: 0.0275 - val_mae: 0.1270 - learning_rate: 3.1641e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.2984e-04 - mae: 0.1296 - val_loss: 0.0272 - val_mae: 0.1268 - learning_rate: 3.1641e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 2.3091e-04 - mae: 0.1293 - val_loss: 0.0274 - val_mae: 0.1265 - learning_rate: 3.1641e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.3405e-04 - mae: 0.1297 - val_loss: 0.0272 - val_mae: 0.1259 - learning_rate: 3.1641e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m624/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.3140e-04 - mae: 0.1296\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 2.3146e-04 - mae: 0.1296 - val_loss: 0.0277 - val_mae: 0.1296 - learning_rate: 3.1641e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 2.2702e-04 - mae: 0.1289 - val_loss: 0.0269 - val_mae: 0.1252 - learning_rate: 2.3730e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.2008e-04 - mae: 0.1281 - val_loss: 0.0267 - val_mae: 0.1250 - learning_rate: 2.3730e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.1526e-04 - mae: 0.1275 - val_loss: 0.0271 - val_mae: 0.1270 - learning_rate: 2.3730e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 2.1807e-04 - mae: 0.1277 - val_loss: 0.0267 - val_mae: 0.1252 - learning_rate: 2.3730e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m626/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.2258e-04 - mae: 0.1277\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 2.2260e-04 - mae: 0.1277 - val_loss: 0.0268 - val_mae: 0.1260 - learning_rate: 2.3730e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 2.1573e-04 - mae: 0.1268 - val_loss: 0.0263 - val_mae: 0.1235 - learning_rate: 1.7798e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 2.1055e-04 - mae: 0.1261 - val_loss: 0.0261 - val_mae: 0.1229 - learning_rate: 1.7798e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.0726e-04 - mae: 0.1253 - val_loss: 0.0260 - val_mae: 0.1228 - learning_rate: 1.7798e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.0708e-04 - mae: 0.1254 - val_loss: 0.0260 - val_mae: 0.1241 - learning_rate: 1.7798e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m628/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.0621e-04 - mae: 0.1262\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0001334838816546835.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 2.0624e-04 - mae: 0.1262 - val_loss: 0.0262 - val_mae: 0.1233 - learning_rate: 1.7798e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 2.0211e-04 - mae: 0.1247 - val_loss: 0.0256 - val_mae: 0.1219 - learning_rate: 1.3348e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.9576e-04 - mae: 0.1237 - val_loss: 0.0254 - val_mae: 0.1216 - learning_rate: 1.3348e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 1.9606e-04 - mae: 0.1242 - val_loss: 0.0257 - val_mae: 0.1218 - learning_rate: 1.3348e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.9630e-04 - mae: 0.1234 - val_loss: 0.0253 - val_mae: 0.1212 - learning_rate: 1.3348e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m626/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.9461e-04 - mae: 0.1236\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00010011290578404441.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.9466e-04 - mae: 0.1237 - val_loss: 0.0253 - val_mae: 0.1214 - learning_rate: 1.3348e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.9331e-04 - mae: 0.1231 - val_loss: 0.0249 - val_mae: 0.1196 - learning_rate: 1.0011e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.8726e-04 - mae: 0.1221 - val_loss: 0.0248 - val_mae: 0.1195 - learning_rate: 1.0011e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.8864e-04 - mae: 0.1222 - val_loss: 0.0249 - val_mae: 0.1191 - learning_rate: 1.0011e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 1.8799e-04 - mae: 0.1219 - val_loss: 0.0246 - val_mae: 0.1193 - learning_rate: 1.0011e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 1.8267e-04 - mae: 0.1209 - val_loss: 0.0245 - val_mae: 0.1188 - learning_rate: 1.0011e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.8627e-04 - mae: 0.1219 - val_loss: 0.0246 - val_mae: 0.1191 - learning_rate: 1.0011e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 1.8439e-04 - mae: 0.1214 - val_loss: 0.0245 - val_mae: 0.1187 - learning_rate: 1.0011e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.8408e-04 - mae: 0.1212 - val_loss: 0.0243 - val_mae: 0.1188 - learning_rate: 1.0011e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.8357e-04 - mae: 0.1208 - val_loss: 0.0242 - val_mae: 0.1178 - learning_rate: 1.0011e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.8272e-04 - mae: 0.1209 - val_loss: 0.0245 - val_mae: 0.1189 - learning_rate: 1.0011e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.8397e-04 - mae: 0.1209 - val_loss: 0.0240 - val_mae: 0.1174 - learning_rate: 1.0011e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 1.8395e-04 - mae: 0.1206 - val_loss: 0.0242 - val_mae: 0.1174 - learning_rate: 1.0011e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 1.8107e-04 - mae: 0.1202 - val_loss: 0.0241 - val_mae: 0.1181 - learning_rate: 1.0011e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.8233e-04 - mae: 0.1209\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 7.508467933803331e-05.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.8234e-04 - mae: 0.1209 - val_loss: 0.0240 - val_mae: 0.1173 - learning_rate: 1.0011e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.7770e-04 - mae: 0.1199 - val_loss: 0.0237 - val_mae: 0.1163 - learning_rate: 7.5085e-05\n",
      "Epoch 67/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.7559e-04 - mae: 0.1187 - val_loss: 0.0234 - val_mae: 0.1154 - learning_rate: 7.5085e-05\n",
      "Epoch 68/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 1.7608e-04 - mae: 0.1186 - val_loss: 0.0233 - val_mae: 0.1157 - learning_rate: 7.5085e-05\n",
      "Epoch 69/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.7400e-04 - mae: 0.1185 - val_loss: 0.0232 - val_mae: 0.1150 - learning_rate: 7.5085e-05\n",
      "Epoch 70/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.7187e-04 - mae: 0.1181 - val_loss: 0.0232 - val_mae: 0.1149 - learning_rate: 7.5085e-05\n",
      "Epoch 71/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 1.6998e-04 - mae: 0.1180 - val_loss: 0.0231 - val_mae: 0.1148 - learning_rate: 7.5085e-05\n",
      "Epoch 72/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 1.7071e-04 - mae: 0.1176 - val_loss: 0.0230 - val_mae: 0.1141 - learning_rate: 7.5085e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.6985e-04 - mae: 0.1176 - val_loss: 0.0233 - val_mae: 0.1148 - learning_rate: 7.5085e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.6769e-04 - mae: 0.1177 - val_loss: 0.0229 - val_mae: 0.1145 - learning_rate: 7.5085e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.6615e-04 - mae: 0.1171 - val_loss: 0.0228 - val_mae: 0.1139 - learning_rate: 7.5085e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.7029e-04 - mae: 0.1173 - val_loss: 0.0228 - val_mae: 0.1145 - learning_rate: 7.5085e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.6736e-04 - mae: 0.1172 - val_loss: 0.0227 - val_mae: 0.1134 - learning_rate: 7.5085e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 1.6906e-04 - mae: 0.1167 - val_loss: 0.0226 - val_mae: 0.1134 - learning_rate: 7.5085e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.6976e-04 - mae: 0.1171 - val_loss: 0.0226 - val_mae: 0.1139 - learning_rate: 7.5085e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.6957e-04 - mae: 0.1170 - val_loss: 0.0225 - val_mae: 0.1127 - learning_rate: 7.5085e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.6639e-04 - mae: 0.1166 - val_loss: 0.0223 - val_mae: 0.1121 - learning_rate: 7.5085e-05\n",
      "Epoch 82/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.6719e-04 - mae: 0.1164 - val_loss: 0.0224 - val_mae: 0.1125 - learning_rate: 7.5085e-05\n",
      "Epoch 83/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.6482e-04 - mae: 0.1166 - val_loss: 0.0223 - val_mae: 0.1120 - learning_rate: 7.5085e-05\n",
      "Epoch 84/200\n",
      "\u001b[1m628/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.6559e-04 - mae: 0.1155\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.6313510867767036e-05.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.6560e-04 - mae: 0.1156 - val_loss: 0.0223 - val_mae: 0.1117 - learning_rate: 7.5085e-05\n",
      "Epoch 85/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.6011e-04 - mae: 0.1148 - val_loss: 0.0216 - val_mae: 0.1104 - learning_rate: 5.6314e-05\n",
      "Epoch 86/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 1.5607e-04 - mae: 0.1144 - val_loss: 0.0216 - val_mae: 0.1100 - learning_rate: 5.6314e-05\n",
      "Epoch 87/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.5450e-04 - mae: 0.1135 - val_loss: 0.0215 - val_mae: 0.1101 - learning_rate: 5.6314e-05\n",
      "Epoch 88/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.5615e-04 - mae: 0.1140 - val_loss: 0.0214 - val_mae: 0.1095 - learning_rate: 5.6314e-05\n",
      "Epoch 89/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.5546e-04 - mae: 0.1142 - val_loss: 0.0214 - val_mae: 0.1096 - learning_rate: 5.6314e-05\n",
      "Epoch 90/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.5393e-04 - mae: 0.1135 - val_loss: 0.0213 - val_mae: 0.1090 - learning_rate: 5.6314e-05\n",
      "Epoch 91/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 1.5295e-04 - mae: 0.1132 - val_loss: 0.0213 - val_mae: 0.1090 - learning_rate: 5.6314e-05\n",
      "Epoch 92/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.5325e-04 - mae: 0.1122 - val_loss: 0.0212 - val_mae: 0.1088 - learning_rate: 5.6314e-05\n",
      "Epoch 93/200\n",
      "\u001b[1m627/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.5403e-04 - mae: 0.1133\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 4.223513315082528e-05.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.5406e-04 - mae: 0.1133 - val_loss: 0.0212 - val_mae: 0.1089 - learning_rate: 5.6314e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.5086e-04 - mae: 0.1118 - val_loss: 0.0208 - val_mae: 0.1079 - learning_rate: 4.2235e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.4767e-04 - mae: 0.1118 - val_loss: 0.0206 - val_mae: 0.1072 - learning_rate: 4.2235e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.5058e-04 - mae: 0.1117 - val_loss: 0.0206 - val_mae: 0.1071 - learning_rate: 4.2235e-05\n",
      "Epoch 97/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.4387e-04 - mae: 0.1106 - val_loss: 0.0206 - val_mae: 0.1070 - learning_rate: 4.2235e-05\n",
      "Epoch 98/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.4536e-04 - mae: 0.1105\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 3.167634986311896e-05.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.4537e-04 - mae: 0.1105 - val_loss: 0.0206 - val_mae: 0.1071 - learning_rate: 4.2235e-05\n",
      "Epoch 99/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.4290e-04 - mae: 0.1100 - val_loss: 0.0203 - val_mae: 0.1063 - learning_rate: 3.1676e-05\n",
      "Epoch 100/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.4134e-04 - mae: 0.1097 - val_loss: 0.0203 - val_mae: 0.1058 - learning_rate: 3.1676e-05\n",
      "Epoch 101/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.4059e-04 - mae: 0.1101 - val_loss: 0.0201 - val_mae: 0.1055 - learning_rate: 3.1676e-05\n",
      "Epoch 102/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.3941e-04 - mae: 0.1094 - val_loss: 0.0200 - val_mae: 0.1052 - learning_rate: 3.1676e-05\n",
      "Epoch 103/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.3844e-04 - mae: 0.1094 - val_loss: 0.0200 - val_mae: 0.1053 - learning_rate: 3.1676e-05\n",
      "Epoch 104/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.3938e-04 - mae: 0.1096 - val_loss: 0.0199 - val_mae: 0.1053 - learning_rate: 3.1676e-05\n",
      "Epoch 105/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.4005e-04 - mae: 0.1094 - val_loss: 0.0199 - val_mae: 0.1045 - learning_rate: 3.1676e-05\n",
      "Epoch 106/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.3711e-04 - mae: 0.1092 - val_loss: 0.0198 - val_mae: 0.1046 - learning_rate: 3.1676e-05\n",
      "Epoch 107/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.3781e-04 - mae: 0.1093 - val_loss: 0.0197 - val_mae: 0.1048 - learning_rate: 3.1676e-05\n",
      "Epoch 108/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.3715e-04 - mae: 0.1089 - val_loss: 0.0198 - val_mae: 0.1044 - learning_rate: 3.1676e-05\n",
      "Epoch 109/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.3497e-04 - mae: 0.1084 - val_loss: 0.0197 - val_mae: 0.1041 - learning_rate: 3.1676e-05\n",
      "Epoch 110/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.3855e-04 - mae: 0.1089 - val_loss: 0.0196 - val_mae: 0.1040 - learning_rate: 3.1676e-05\n",
      "Epoch 111/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.3646e-04 - mae: 0.1089 - val_loss: 0.0196 - val_mae: 0.1040 - learning_rate: 3.1676e-05\n",
      "Epoch 112/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.3499e-04 - mae: 0.1079 - val_loss: 0.0196 - val_mae: 0.1039 - learning_rate: 3.1676e-05\n",
      "Epoch 113/200\n",
      "\u001b[1m628/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.3775e-04 - mae: 0.1087\n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 2.3757263079460245e-05.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.3776e-04 - mae: 0.1087 - val_loss: 0.0196 - val_mae: 0.1038 - learning_rate: 3.1676e-05\n",
      "Epoch 114/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.3404e-04 - mae: 0.1076 - val_loss: 0.0193 - val_mae: 0.1028 - learning_rate: 2.3757e-05\n",
      "Epoch 115/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.3106e-04 - mae: 0.1067 - val_loss: 0.0192 - val_mae: 0.1028 - learning_rate: 2.3757e-05\n",
      "Epoch 116/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.3223e-04 - mae: 0.1070 - val_loss: 0.0191 - val_mae: 0.1024 - learning_rate: 2.3757e-05\n",
      "Epoch 117/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.3135e-04 - mae: 0.1065 - val_loss: 0.0191 - val_mae: 0.1021 - learning_rate: 2.3757e-05\n",
      "Epoch 118/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.3071e-04 - mae: 0.1066 - val_loss: 0.0191 - val_mae: 0.1022 - learning_rate: 2.3757e-05\n",
      "Epoch 119/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 1.3029e-04 - mae: 0.1063 - val_loss: 0.0190 - val_mae: 0.1019 - learning_rate: 2.3757e-05\n",
      "Epoch 120/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 1.3230e-04 - mae: 0.1072 - val_loss: 0.0189 - val_mae: 0.1018 - learning_rate: 2.3757e-05\n",
      "Epoch 121/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.3138e-04 - mae: 0.1060 - val_loss: 0.0189 - val_mae: 0.1015 - learning_rate: 2.3757e-05\n",
      "Epoch 122/200\n",
      "\u001b[1m626/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.3075e-04 - mae: 0.1069\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 1.781794799171621e-05.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.3077e-04 - mae: 0.1069 - val_loss: 0.0190 - val_mae: 0.1018 - learning_rate: 2.3757e-05\n",
      "Epoch 123/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2972e-04 - mae: 0.1068 - val_loss: 0.0188 - val_mae: 0.1008 - learning_rate: 1.7818e-05\n",
      "Epoch 124/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2479e-04 - mae: 0.1056 - val_loss: 0.0188 - val_mae: 0.1009 - learning_rate: 1.7818e-05\n",
      "Epoch 125/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.2757e-04 - mae: 0.1060 - val_loss: 0.0187 - val_mae: 0.1005 - learning_rate: 1.7818e-05\n",
      "Epoch 126/200\n",
      "\u001b[1m626/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.2626e-04 - mae: 0.1052\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 1.3363460311666131e-05.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.2628e-04 - mae: 0.1052 - val_loss: 0.0187 - val_mae: 0.1009 - learning_rate: 1.7818e-05\n",
      "Epoch 127/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2524e-04 - mae: 0.1049 - val_loss: 0.0186 - val_mae: 0.1002 - learning_rate: 1.3363e-05\n",
      "Epoch 128/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2687e-04 - mae: 0.1050 - val_loss: 0.0185 - val_mae: 0.1002 - learning_rate: 1.3363e-05\n",
      "Epoch 129/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2220e-04 - mae: 0.1043 - val_loss: 0.0184 - val_mae: 0.0998 - learning_rate: 1.3363e-05\n",
      "Epoch 130/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2243e-04 - mae: 0.1042 - val_loss: 0.0184 - val_mae: 0.1000 - learning_rate: 1.3363e-05\n",
      "Epoch 131/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2374e-04 - mae: 0.1049 - val_loss: 0.0184 - val_mae: 0.0998 - learning_rate: 1.3363e-05\n",
      "Epoch 132/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.2342e-04 - mae: 0.1042\n",
      "Epoch 132: ReduceLROnPlateau reducing learning rate to 1.0022595233749598e-05.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2343e-04 - mae: 0.1042 - val_loss: 0.0184 - val_mae: 0.0998 - learning_rate: 1.3363e-05\n",
      "Epoch 133/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2336e-04 - mae: 0.1040 - val_loss: 0.0183 - val_mae: 0.0994 - learning_rate: 1.0023e-05\n",
      "Epoch 134/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.2201e-04 - mae: 0.1032 - val_loss: 0.0183 - val_mae: 0.0995 - learning_rate: 1.0023e-05\n",
      "Epoch 135/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.2043e-04 - mae: 0.1035 - val_loss: 0.0182 - val_mae: 0.0993 - learning_rate: 1.0023e-05\n",
      "Epoch 136/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2112e-04 - mae: 0.1035 - val_loss: 0.0182 - val_mae: 0.0988 - learning_rate: 1.0023e-05\n",
      "Epoch 137/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2219e-04 - mae: 0.1034 - val_loss: 0.0182 - val_mae: 0.0992 - learning_rate: 1.0023e-05\n",
      "Epoch 138/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.2113e-04 - mae: 0.1034 - val_loss: 0.0182 - val_mae: 0.0991 - learning_rate: 1.0023e-05\n",
      "Epoch 139/200\n",
      "\u001b[1m628/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.2038e-04 - mae: 0.1032\n",
      "Epoch 139: ReduceLROnPlateau reducing learning rate to 7.516946425312199e-06.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.2038e-04 - mae: 0.1032 - val_loss: 0.0181 - val_mae: 0.0989 - learning_rate: 1.0023e-05\n",
      "Epoch 140/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.1881e-04 - mae: 0.1028 - val_loss: 0.0181 - val_mae: 0.0989 - learning_rate: 7.5169e-06\n",
      "Epoch 141/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2006e-04 - mae: 0.1032 - val_loss: 0.0181 - val_mae: 0.0986 - learning_rate: 7.5169e-06\n",
      "Epoch 142/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2134e-04 - mae: 0.1029 - val_loss: 0.0180 - val_mae: 0.0984 - learning_rate: 7.5169e-06\n",
      "Epoch 143/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1884e-04 - mae: 0.1029 - val_loss: 0.0180 - val_mae: 0.0984 - learning_rate: 7.5169e-06\n",
      "Epoch 144/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.2017e-04 - mae: 0.1032 - val_loss: 0.0179 - val_mae: 0.0983 - learning_rate: 7.5169e-06\n",
      "Epoch 145/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.1781e-04 - mae: 0.1026 - val_loss: 0.0179 - val_mae: 0.0980 - learning_rate: 7.5169e-06\n",
      "Epoch 146/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1904e-04 - mae: 0.1026 - val_loss: 0.0179 - val_mae: 0.0981 - learning_rate: 7.5169e-06\n",
      "Epoch 147/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1876e-04 - mae: 0.1026 - val_loss: 0.0179 - val_mae: 0.0982 - learning_rate: 7.5169e-06\n",
      "Epoch 148/200\n",
      "\u001b[1m625/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1914e-04 - mae: 0.1027\n",
      "Epoch 148: ReduceLROnPlateau reducing learning rate to 5.637709818984149e-06.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1915e-04 - mae: 0.1027 - val_loss: 0.0180 - val_mae: 0.0982 - learning_rate: 7.5169e-06\n",
      "Epoch 149/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1881e-04 - mae: 0.1025 - val_loss: 0.0179 - val_mae: 0.0978 - learning_rate: 5.6377e-06\n",
      "Epoch 150/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1762e-04 - mae: 0.1020 - val_loss: 0.0179 - val_mae: 0.0978 - learning_rate: 5.6377e-06\n",
      "Epoch 151/200\n",
      "\u001b[1m624/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1731e-04 - mae: 0.1017\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 4.228282364238112e-06.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1732e-04 - mae: 0.1017 - val_loss: 0.0178 - val_mae: 0.0978 - learning_rate: 5.6377e-06\n",
      "Epoch 152/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1787e-04 - mae: 0.1020 - val_loss: 0.0178 - val_mae: 0.0976 - learning_rate: 4.2283e-06\n",
      "Epoch 153/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1685e-04 - mae: 0.1016 - val_loss: 0.0178 - val_mae: 0.0975 - learning_rate: 4.2283e-06\n",
      "Epoch 154/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1653e-04 - mae: 0.1015 - val_loss: 0.0177 - val_mae: 0.0975 - learning_rate: 4.2283e-06\n",
      "Epoch 155/200\n",
      "\u001b[1m626/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1588e-04 - mae: 0.1016\n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 3.1712116879134555e-06.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1589e-04 - mae: 0.1016 - val_loss: 0.0177 - val_mae: 0.0975 - learning_rate: 4.2283e-06\n",
      "Epoch 156/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 1.1522e-04 - mae: 0.1012 - val_loss: 0.0177 - val_mae: 0.0974 - learning_rate: 3.1712e-06\n",
      "Epoch 157/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1687e-04 - mae: 0.1015 - val_loss: 0.0177 - val_mae: 0.0973 - learning_rate: 3.1712e-06\n",
      "Epoch 158/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1733e-04 - mae: 0.1018 - val_loss: 0.0177 - val_mae: 0.0973 - learning_rate: 3.1712e-06\n",
      "Epoch 159/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.1632e-04 - mae: 0.1017 - val_loss: 0.0177 - val_mae: 0.0972 - learning_rate: 3.1712e-06\n",
      "Epoch 160/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1575e-04 - mae: 0.1015 - val_loss: 0.0176 - val_mae: 0.0971 - learning_rate: 3.1712e-06\n",
      "Epoch 161/200\n",
      "\u001b[1m627/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1560e-04 - mae: 0.1017\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 2.3784086806699634e-06.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1560e-04 - mae: 0.1017 - val_loss: 0.0176 - val_mae: 0.0972 - learning_rate: 3.1712e-06\n",
      "Epoch 162/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.1433e-04 - mae: 0.1007 - val_loss: 0.0176 - val_mae: 0.0971 - learning_rate: 2.3784e-06\n",
      "Epoch 163/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1429e-04 - mae: 0.1017 - val_loss: 0.0176 - val_mae: 0.0969 - learning_rate: 2.3784e-06\n",
      "Epoch 164/200\n",
      "\u001b[1m628/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1353e-04 - mae: 0.1016\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 1.7838065105024725e-06.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1354e-04 - mae: 0.1016 - val_loss: 0.0176 - val_mae: 0.0969 - learning_rate: 2.3784e-06\n",
      "Epoch 165/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1474e-04 - mae: 0.1012 - val_loss: 0.0176 - val_mae: 0.0969 - learning_rate: 1.7838e-06\n",
      "Epoch 166/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1324e-04 - mae: 0.1005 - val_loss: 0.0176 - val_mae: 0.0969 - learning_rate: 1.7838e-06\n",
      "Epoch 167/200\n",
      "\u001b[1m624/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1406e-04 - mae: 0.1011\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 1.3378548828768544e-06.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1407e-04 - mae: 0.1011 - val_loss: 0.0176 - val_mae: 0.0969 - learning_rate: 1.7838e-06\n",
      "Epoch 168/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.1672e-04 - mae: 0.1013 - val_loss: 0.0176 - val_mae: 0.0968 - learning_rate: 1.3379e-06\n",
      "Epoch 169/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1357e-04 - mae: 0.1010 - val_loss: 0.0176 - val_mae: 0.0968 - learning_rate: 1.3379e-06\n",
      "Epoch 170/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1415e-04 - mae: 0.1005 - val_loss: 0.0176 - val_mae: 0.0967 - learning_rate: 1.3379e-06\n",
      "Epoch 171/200\n",
      "\u001b[1m628/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1623e-04 - mae: 0.1009\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 1.0033911621576408e-06.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1623e-04 - mae: 0.1009 - val_loss: 0.0175 - val_mae: 0.0967 - learning_rate: 1.3379e-06\n",
      "Epoch 172/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.1473e-04 - mae: 0.1009 - val_loss: 0.0175 - val_mae: 0.0967 - learning_rate: 1.0034e-06\n",
      "Epoch 173/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1526e-04 - mae: 0.1015 - val_loss: 0.0175 - val_mae: 0.0967 - learning_rate: 1.0034e-06\n",
      "Epoch 174/200\n",
      "\u001b[1m626/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.1126e-04 - mae: 0.1003\n",
      "Epoch 174: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1128e-04 - mae: 0.1003 - val_loss: 0.0175 - val_mae: 0.0967 - learning_rate: 1.0034e-06\n",
      "Epoch 175/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1379e-04 - mae: 0.1010 - val_loss: 0.0175 - val_mae: 0.0967 - learning_rate: 1.0000e-06\n",
      "Epoch 176/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1414e-04 - mae: 0.1012 - val_loss: 0.0175 - val_mae: 0.0967 - learning_rate: 1.0000e-06\n",
      "Epoch 177/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1326e-04 - mae: 0.1005 - val_loss: 0.0175 - val_mae: 0.0967 - learning_rate: 1.0000e-06\n",
      "Epoch 178/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1385e-04 - mae: 0.1004 - val_loss: 0.0175 - val_mae: 0.0967 - learning_rate: 1.0000e-06\n",
      "Epoch 179/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1335e-04 - mae: 0.1001 - val_loss: 0.0175 - val_mae: 0.0967 - learning_rate: 1.0000e-06\n",
      "Epoch 180/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1453e-04 - mae: 0.1009 - val_loss: 0.0175 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
      "Epoch 181/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.1328e-04 - mae: 0.1008 - val_loss: 0.0175 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
      "Epoch 182/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.1273e-04 - mae: 0.1002 - val_loss: 0.0175 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
      "Epoch 183/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1334e-04 - mae: 0.1008 - val_loss: 0.0175 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
      "Epoch 184/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.1477e-04 - mae: 0.1006 - val_loss: 0.0175 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
      "Epoch 185/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1218e-04 - mae: 0.1005 - val_loss: 0.0175 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
      "Epoch 186/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1282e-04 - mae: 0.1010 - val_loss: 0.0175 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
      "Epoch 187/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1292e-04 - mae: 0.1005 - val_loss: 0.0175 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
      "Epoch 188/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1247e-04 - mae: 0.1002 - val_loss: 0.0175 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
      "Epoch 189/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1444e-04 - mae: 0.1009 - val_loss: 0.0175 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
      "Epoch 190/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1209e-04 - mae: 0.1004 - val_loss: 0.0175 - val_mae: 0.0965 - learning_rate: 1.0000e-06\n",
      "Epoch 191/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1273e-04 - mae: 0.1010 - val_loss: 0.0175 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
      "Epoch 192/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 1.1441e-04 - mae: 0.1008 - val_loss: 0.0175 - val_mae: 0.0965 - learning_rate: 1.0000e-06\n",
      "Epoch 193/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.1217e-04 - mae: 0.1006 - val_loss: 0.0175 - val_mae: 0.0965 - learning_rate: 1.0000e-06\n",
      "Epoch 194/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1466e-04 - mae: 0.1007 - val_loss: 0.0175 - val_mae: 0.0965 - learning_rate: 1.0000e-06\n",
      "Epoch 195/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1362e-04 - mae: 0.1006 - val_loss: 0.0175 - val_mae: 0.0965 - learning_rate: 1.0000e-06\n",
      "Epoch 196/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1513e-04 - mae: 0.1012 - val_loss: 0.0175 - val_mae: 0.0965 - learning_rate: 1.0000e-06\n",
      "Epoch 197/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.1357e-04 - mae: 0.1005 - val_loss: 0.0175 - val_mae: 0.0966 - learning_rate: 1.0000e-06\n",
      "Epoch 198/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1153e-04 - mae: 0.1006 - val_loss: 0.0175 - val_mae: 0.0965 - learning_rate: 1.0000e-06\n",
      "Epoch 199/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1252e-04 - mae: 0.1010 - val_loss: 0.0175 - val_mae: 0.0965 - learning_rate: 1.0000e-06\n",
      "Epoch 200/200\n",
      "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 1.1404e-04 - mae: 0.1011 - val_loss: 0.0175 - val_mae: 0.0965 - learning_rate: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.75, patience=3, min_lr=0.000001, verbose=1)\n",
    "\n",
    "# print(X_train_array.T.shape, y_train.shape)\n",
    "# print(len(X_test_array), len(y_test))\n",
    "# x_train = [X_train_array[0], X_train_array[1]]\n",
    "# x_test = [X_test_array[0], X_test_array[1]]\n",
    "\n",
    "weights = 1.0 / train_data.groupby('userId')['rating'].count()\n",
    "sample_weights = train_data['userId'].map(weights)\n",
    "\n",
    "y_train_normalized = y_train_class / 10.0\n",
    "y_val_normalized = y_val_class / 10.0\n",
    "# print(X_train_array, y_train_normalized)\n",
    "# print(X_val_array[0].shape, X_val_array[1].shape, y_val_class.shape)\n",
    "history = model.fit(x=X_train_array, y=y_train_normalized, sample_weight=sample_weights, batch_size=128, epochs=200, verbose=1,shuffle=True, validation_data=(X_val_array, y_val_normalized), callbacks=[reduce_lr])\n",
    "# history = model.fit(x=X_train_array,y=y_train_class, batch_size=128, epochs=70, verbose=1,shuffle=True, callbacks=[reduce_lr])\n",
    "\n",
    "# without mappings loss = 1.134 accuracy = 0.3664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"][5:])\n",
    "# plt.plot(history.history[\"val_loss\"][5:])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2514/2514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1000us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.81948256],\n",
       "       [0.7730682 ],\n",
       "       [0.9024277 ],\n",
       "       ...,\n",
       "       [0.60676754],\n",
       "       [0.6476829 ],\n",
       "       [0.7907988 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(x = X_train_array)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80419,) [0 0 0 ... 0 0 0]\n",
      "Accuracy: 0.013218269314465488\n"
     ]
    }
   ],
   "source": [
    "predicted_ratings = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(predicted_ratings.shape, predicted_ratings)\n",
    "unique, counts = np.unique(predicted_ratings, return_counts=True)\n",
    "# print(dict(zip(unique, counts)))\n",
    "# train_data['rating'].value_counts()\n",
    "\n",
    "# for (i, pred) in enumerate(predicted_ratings):\n",
    "#     print(f\"Predicted: {pred} Actual: {y_train_class[i]}\")\n",
    "\n",
    "def convert_rating(rating):\n",
    "    return label_encoder.inverse_transform(rating)\n",
    "    \n",
    "converted_predicted_ratings = convert_rating(predicted_ratings)\n",
    "converted_actual_ratings = convert_rating(y_train_class)\n",
    "\n",
    "# for (i, pred) in enumerate(converted_predicted_ratings):\n",
    "#     print(f\"Predicted: {pred} Actual: {converted_actual_ratings[i]}\")\n",
    "\n",
    "correct, total = 0, 0\n",
    "for (i, pred) in enumerate(converted_predicted_ratings):\n",
    "    if pred == converted_actual_ratings[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Accuracy: {correct/total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwatched_movies(ratings):\n",
    "    all_movie_ids = set(ratings['movieId'])\n",
    "    user_to_unwatched_movies = {}\n",
    "\n",
    "    for user_id in ratings['userId'].unique():\n",
    "        user = ratings[ratings['userId'] == user_id]\n",
    "        watched_movies = set(user['movieId'])\n",
    "        unwatched_movies = all_movie_ids - watched_movies\n",
    "        user_to_unwatched_movies[user_id] = unwatched_movies\n",
    "\n",
    "    return user_to_unwatched_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(model, user_to_idx, movie_to_idx, n=10):\n",
    "    unwatched = unwatched_movies(train_data)\n",
    "    recommendations = {}\n",
    "\n",
    "    for user_id, movies in unwatched.items():\n",
    "        user_idx = user_to_idx[user_id]\n",
    "        movie_indices = [movie_to_idx[movie_id] for movie_id in movies]\n",
    "        \n",
    "        user_array = np.array([user_idx] * len(movie_indices))\n",
    "        movie_array = np.array(movie_indices)\n",
    "        \n",
    "        predictions = model.predict([user_array, movie_array]).flatten()\n",
    "        top_n_indices = np.argsort(predictions)[::-1][:n]\n",
    "\n",
    "        top_n_movie_ids = [int(id_movie_mapping[movie_indices[i]]) for i in top_n_indices]\n",
    "        print(f\"User {user_id}: {top_n_movie_ids}\")\n",
    "        \n",
    "        recommendations[user_id] = top_n_movie_ids\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "top_10_recommendations = recommend_movies(model, user_id_mapping, movie_id_mapping, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_recommend_movies_v2(model, user_to_idx, movie_to_idx, train_data, n=10, \n",
    "                                 rating_threshold=3.5, min_ratings=10):\n",
    "    \"\"\"\n",
    "    Enhanced recommendation function for the second version:\n",
    "    1. Considers movie popularity and rating distribution\n",
    "    2. Filters out movies with too few ratings\n",
    "    3. Uses rating threshold to determine \"liked\" movies\n",
    "    4. Implements candidate selection\n",
    "    \"\"\"\n",
    "    # Calculate movie statistics (count, mean, std)\n",
    "    movie_stats = train_data.groupby('movieId').agg({\n",
    "        'rating': ['count', 'mean', 'std']\n",
    "    }).droplevel(0, axis=1)\n",
    "    \n",
    "    # Filter movies with enough ratings\n",
    "    valid_movies = set(movie_stats[movie_stats['count'] >= min_ratings].index)\n",
    "    \n",
    "    # Calculate user preferences (movies rated >= rating_threshold)\n",
    "    user_preferences = {}\n",
    "    for user_id in train_data['userId'].unique():\n",
    "        user_ratings = train_data[train_data['userId'] == user_id]\n",
    "        liked_movies = user_ratings[user_ratings['rating'] >= rating_threshold]['movieId']\n",
    "        \n",
    "        if len(liked_movies) > 0:\n",
    "            user_preferences[user_id] = liked_movies.tolist()\n",
    "    \n",
    "    unwatched = unwatched_movies(train_data)\n",
    "    recommendations = {}\n",
    "\n",
    "    for user_id, movies in unwatched.items():\n",
    "        # Only consider valid unwatched movies\n",
    "        user_unwatched = set(movies).intersection(valid_movies)\n",
    "        \n",
    "        if not user_unwatched:\n",
    "            continue\n",
    "        \n",
    "        user_idx = user_to_idx[user_id]\n",
    "        movie_indices = [movie_to_idx[movie_id] for movie_id in user_unwatched]\n",
    "        \n",
    "        user_array = np.array([user_idx] * len(movie_indices))\n",
    "        movie_array = np.array(movie_indices)\n",
    "        \n",
    "        # Get predictions for unwatched movies\n",
    "        predictions = model.predict([user_array, movie_array]).flatten()\n",
    "        \n",
    "        # Calculate confidence and popularity scores\n",
    "        predictions_with_scores = []\n",
    "        for movie_id, pred_rating in zip(user_unwatched, predictions):\n",
    "            movie_idx = movie_to_idx[movie_id]\n",
    "            \n",
    "            # Confidence score based on the number of ratings\n",
    "            confidence = 1 - (1 / (1 + movie_stats.loc[movie_id, 'count'] / 100))\n",
    "            \n",
    "            # Popularity score normalized between 0 and 1\n",
    "            popularity = movie_stats.loc[movie_id, 'count'] / movie_stats['count'].max()\n",
    "            \n",
    "            # Adjusted score\n",
    "            adjusted_score = (0.7 * pred_rating + \n",
    "                              0.2 * confidence + \n",
    "                              0.1 * popularity)\n",
    "            \n",
    "            predictions_with_scores.append((movie_id, adjusted_score))\n",
    "        \n",
    "        # Sort by adjusted score and get top N\n",
    "        top_n_movie_ids = [\n",
    "            movie_id for movie_id, _ in sorted(predictions_with_scores, key=lambda x: x[1], reverse=True)[:n]\n",
    "        ]\n",
    "        \n",
    "        recommendations[user_id] = top_n_movie_ids\n",
    "        print(f\"User {user_id}: {top_n_movie_ids}\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Example usage:\n",
    "top_10_recommendations_v2 = enhanced_recommend_movies_v2(model, user_id_mapping, movie_id_mapping, train_data, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 1: [318, 296, 260, 589, 4993, 858, 150, 5952, 7153, 2762]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "User 2: [296, 356, 2571, 260, 593, 2858, 110, 150, 1198, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 3: [356, 318, 593, 47, 296, 32, 2571, 1210, 608, 2959]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 4: [318, 356, 2571, 260, 110, 2858, 589, 480, 527, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 5: [356, 2571, 593, 260, 1196, 2858, 4993, 2959, 480, 589]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step\n",
      "User 6: [296, 2571, 260, 2858, 2959, 1196, 780, 4993, 1198, 5952]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 7: [318, 296, 2571, 480, 589, 110, 2959, 47, 780, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 8: [318, 356, 2571, 260, 2959, 527, 1196, 589, 2858, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "User 9: [356, 318, 296, 2571, 593, 260, 110, 2959, 858, 47]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 10: [318, 2571, 260, 480, 589, 110, 593, 780, 1198, 47]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step\n",
      "User 11: [318, 296, 2571, 260, 110, 527, 1196, 2959, 2858, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 12: [356, 318, 296, 2571, 593, 260, 110, 2959, 589, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 13: [318, 356, 296, 593, 260, 110, 2959, 1196, 527, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 14: [2571, 260, 589, 2858, 1196, 2959, 4993, 1198, 527, 5952]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 15: [593, 110, 2959, 480, 32, 50, 150, 7153, 457, 2762]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 16: [593, 260, 480, 110, 589, 527, 47, 5952, 150, 858]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 17: [318, 593, 110, 480, 2762, 377, 32, 588, 608, 457]\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 18: [110, 2959, 2858, 1198, 150, 457, 4306, 364, 592, 1]\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 19: [356, 318, 296, 593, 2571, 260, 1196, 527, 110, 1198]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 20: [318, 356, 296, 2571, 593, 260, 110, 589, 2858, 480]\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 21: [318, 2571, 593, 110, 480, 589, 1198, 780, 150, 377]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 22: [356, 296, 2571, 32, 260, 1210, 593, 608, 47, 480]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step\n",
      "User 23: [356, 318, 2571, 593, 260, 110, 2858, 1196, 2959, 480]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 24: [593, 260, 110, 2959, 589, 480, 2858, 1196, 4993, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 25: [356, 318, 296, 593, 110, 2959, 1196, 858, 589, 150]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 26: [318, 2571, 260, 110, 480, 2858, 589, 2959, 4993, 1196]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 27: [318, 296, 2571, 593, 110, 480, 589, 2858, 1196, 2959]\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 28: [356, 2571, 2959, 1198, 110, 150, 47, 1270, 377, 364]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step\n",
      "User 29: [296, 2571, 593, 260, 110, 589, 1196, 2858, 2959, 480]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 30: [356, 296, 593, 2959, 527, 858, 150, 47, 50, 480]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 31: [356, 318, 2571, 296, 593, 260, 110, 589, 480, 2858]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step\n",
      "User 32: [356, 2571, 110, 2858, 527, 2959, 589, 480, 150, 4993]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 33: [260, 110, 2959, 2858, 1196, 480, 4993, 7153, 5952, 1198]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step\n",
      "User 34: [480, 296, 260, 780, 593, 1198, 2858, 592, 1210, 32]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 35: [318, 356, 296, 2571, 593, 260, 110, 2959, 527, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 36: [356, 296, 260, 2571, 32, 593, 608, 2959, 858, 1210]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 37: [2571, 260, 110, 589, 480, 2858, 780, 1198, 2959, 1196]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 38: [296, 318, 2571, 260, 2858, 1196, 1198, 2959, 4993, 589]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 39: [318, 356, 296, 2571, 593, 110, 2959, 527, 1196, 2858]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 40: [318, 356, 2571, 593, 260, 2959, 527, 1196, 4993, 858]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 41: [318, 260, 527, 1198, 110, 1196, 480, 32, 589, 1210]\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 42: [296, 589, 480, 2858, 4993, 5952, 1198, 7153, 50, 2028]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 43: [2571, 296, 593, 260, 110, 589, 780, 2959, 2858, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 44: [296, 318, 356, 2571, 593, 110, 1196, 527, 2858, 2959]\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 45: [318, 593, 260, 527, 480, 4993, 1198, 47, 7153, 5952]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 46: [318, 356, 296, 2571, 260, 589, 2959, 527, 480, 4993]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 47: [296, 260, 2571, 1196, 2858, 32, 527, 110, 4993, 7153]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 48: [356, 318, 296, 2571, 593, 260, 110, 2959, 480, 589]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 49: [318, 296, 2571, 593, 260, 110, 589, 2959, 2858, 480]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 50: [318, 2959, 260, 527, 1210, 47, 2762, 480, 4993, 2858]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 51: [318, 356, 296, 2571, 110, 2858, 2959, 480, 4993, 1198]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 52: [356, 296, 593, 110, 589, 480, 2858, 4993, 527, 780]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 53: [318, 356, 2571, 296, 593, 260, 110, 589, 2959, 480]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "User 54: [2571, 260, 2959, 2858, 1198, 527, 2762, 1196, 1210, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 55: [356, 296, 318, 593, 2571, 858, 47, 260, 2959, 608]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 56: [2571, 260, 2959, 589, 480, 1196, 527, 4993, 47, 780]\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 57: [318, 260, 2959, 4993, 7153, 5952, 47, 1210, 150, 4306]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 58: [318, 2571, 593, 260, 2959, 1196, 2858, 4993, 1198, 7153]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "User 59: [356, 318, 296, 2959, 150, 47, 4993, 2858, 1198, 7153]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 60: [356, 296, 2571, 593, 260, 110, 2959, 1196, 589, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step\n",
      "User 61: [356, 296, 2571, 260, 110, 2959, 527, 1196, 2858, 858]\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step\n",
      "User 62: [356, 110, 589, 480, 2858, 4993, 150, 858, 7153, 50]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step\n",
      "User 63: [296, 593, 110, 527, 592, 2762, 457, 377, 590, 380]\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step\n",
      "User 64: [2571, 593, 589, 1198, 150, 858, 1270, 608, 592, 457]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 65: [296, 356, 2571, 593, 260, 110, 2858, 1196, 1198, 589]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step\n",
      "User 66: [593, 110, 2959, 527, 150, 858, 2858, 7153, 588, 364]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step\n",
      "User 67: [593, 260, 110, 480, 1198, 1196, 2959, 150, 527, 780]\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 68: [318, 32, 780, 4306, 1270, 588, 380, 6377, 1214, 1704]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step\n",
      "User 69: [318, 110, 2959, 527, 589, 4993, 858, 2858, 480, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step\n",
      "User 70: [296, 2571, 593, 260, 110, 589, 480, 2959, 1196, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step\n",
      "User 71: [356, 318, 296, 2571, 593, 480, 110, 2959, 4993, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "User 72: [318, 2959, 480, 1198, 4993, 150, 7153, 5952, 1210, 2028]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 73: [318, 356, 296, 2571, 593, 110, 589, 480, 2959, 1196]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 74: [318, 356, 2571, 593, 260, 110, 589, 2959, 1196, 527]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 75: [356, 318, 296, 2571, 593, 260, 480, 110, 2959, 1196]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step\n",
      "User 76: [356, 2858, 1198, 527, 50, 32, 150, 110, 1210, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 77: [318, 356, 296, 593, 110, 589, 480, 2858, 2959, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 78: [260, 589, 110, 2959, 47, 4993, 1196, 5952, 7153, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 79: [356, 318, 296, 593, 110, 2959, 4993, 527, 858, 150]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 80: [356, 296, 260, 110, 589, 480, 2858, 527, 1196, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 81: [356, 2571, 260, 296, 593, 589, 2858, 1198, 1210, 2959]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 82: [318, 296, 2571, 593, 2959, 2858, 527, 150, 377, 2762]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 83: [296, 260, 2571, 593, 1196, 110, 2858, 4993, 527, 32]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 84: [2571, 110, 2959, 2858, 4993, 589, 47, 7153, 5952, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 85: [296, 318, 356, 2571, 593, 260, 110, 1196, 480, 589]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 86: [356, 296, 593, 110, 589, 2858, 1198, 150, 527, 47]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 87: [318, 356, 296, 2571, 593, 260, 110, 2959, 589, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "User 88: [296, 356, 2571, 593, 260, 110, 1196, 527, 2858, 2959]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 89: [318, 2571, 296, 260, 480, 589, 593, 110, 780, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 90: [318, 356, 296, 2571, 593, 110, 527, 1196, 2959, 2858]\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 91: [318, 593, 2858, 1196, 527, 1198, 32, 592, 457, 1580]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 92: [356, 318, 296, 2571, 593, 260, 110, 2959, 4993, 527]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 93: [2571, 296, 589, 2959, 780, 1196, 2858, 1198, 4993, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step\n",
      "User 94: [2571, 593, 260, 110, 1196, 2858, 2959, 527, 4993, 7153]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 95: [318, 356, 296, 593, 110, 589, 2959, 527, 2858, 4993]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step\n",
      "User 96: [318, 356, 593, 1196, 2959, 4993, 7153, 5952, 527, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 97: [318, 356, 296, 260, 110, 589, 2959, 480, 1196, 2858]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 98: [318, 296, 2571, 260, 110, 2858, 1196, 2959, 527, 589]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 99: [356, 2571, 593, 260, 110, 2959, 4993, 589, 858, 1196]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 100: [318, 296, 2571, 593, 260, 110, 480, 589, 2858, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 101: [318, 356, 296, 2571, 593, 260, 110, 480, 589, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "User 102: [2571, 593, 260, 110, 2858, 1196, 527, 2959, 4993, 1198]\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 103: [296, 110, 589, 2858, 47, 1198, 2762, 50, 780, 588]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 104: [318, 296, 2571, 593, 480, 589, 2858, 1196, 1198, 2959]\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 105: [480, 1196, 4993, 150, 780, 2028, 1210, 588, 377, 1270]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 106: [356, 296, 593, 260, 110, 527, 1196, 2858, 2959, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 107: [356, 2571, 593, 260, 589, 2959, 480, 527, 4993, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "User 108: [296, 356, 318, 260, 593, 2858, 110, 150, 527, 1196]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 109: [2571, 260, 110, 2959, 1196, 527, 2858, 4993, 858, 7153]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "User 110: [356, 318, 593, 260, 2858, 1196, 150, 2959, 858, 50]\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 111: [296, 260, 110, 589, 1198, 150, 1196, 592, 527, 1210]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 112: [356, 318, 296, 2959, 110, 5952, 858, 589, 1196, 2762]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 113: [296, 318, 2571, 110, 150, 1196, 527, 1198, 50, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 114: [356, 296, 318, 2571, 32, 47, 2959, 527, 608, 858]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 115: [318, 356, 2571, 2959, 527, 4993, 858, 47, 150, 5952]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 116: [318, 2571, 260, 296, 110, 2858, 1210, 592, 32, 457]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 117: [2571, 260, 2959, 2858, 1196, 4993, 1198, 5952, 7153, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 118: [356, 318, 2571, 593, 260, 110, 2959, 2858, 480, 150]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 119: [356, 593, 260, 110, 589, 2959, 480, 527, 1196, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 120: [296, 318, 356, 2571, 593, 110, 527, 1196, 2858, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 121: [593, 2571, 260, 110, 1196, 2858, 2959, 1198, 480, 589]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 122: [593, 110, 589, 527, 2858, 4993, 858, 150, 1198, 5952]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 123: [356, 296, 593, 110, 480, 589, 780, 1198, 150, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 124: [2571, 593, 527, 2959, 1196, 150, 858, 47, 4993, 1198]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 125: [356, 260, 480, 4993, 2858, 1196, 527, 5952, 7153, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 126: [296, 2571, 593, 260, 1196, 527, 2959, 589, 4993, 480]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 127: [296, 318, 356, 260, 2571, 593, 2858, 32, 50, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 128: [356, 318, 296, 2571, 2959, 589, 4993, 480, 527, 150]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 129: [318, 356, 296, 593, 589, 2959, 527, 1198, 150, 780]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 130: [356, 296, 2571, 593, 260, 589, 480, 2858, 2959, 1198]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 131: [356, 260, 1196, 2959, 527, 32, 480, 1210, 1198, 589]\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 132: [2571, 527, 32, 480, 2762, 589, 110, 608, 1198, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 133: [2571, 260, 1196, 4993, 2959, 7153, 589, 5952, 2858, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 134: [318, 356, 2571, 260, 589, 1196, 2858, 527, 2959, 4993]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "User 135: [356, 110, 2959, 589, 150, 7153, 50, 5952, 608, 1270]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 136: [356, 318, 2571, 260, 480, 589, 110, 2858, 780, 4993]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 137: [2858, 527, 4993, 47, 50, 32, 364, 457, 592, 3578]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 138: [356, 318, 296, 2571, 593, 260, 110, 2959, 1196, 4993]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 139: [356, 318, 2571, 296, 593, 32, 480, 2959, 1210, 589]\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 140: [318, 593, 260, 589, 2959, 32, 2762, 592, 590, 608]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 141: [296, 593, 2858, 527, 7153, 50, 589, 2762, 32, 1210]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 142: [318, 296, 2571, 593, 260, 110, 2959, 1196, 2858, 589]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 143: [356, 318, 2571, 296, 260, 593, 589, 480, 110, 780]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 144: [318, 260, 110, 589, 2959, 1196, 1198, 858, 50, 2028]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 145: [356, 2571, 593, 260, 110, 589, 480, 1196, 2858, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 146: [296, 318, 356, 2571, 593, 260, 110, 1196, 527, 2858]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 147: [356, 318, 296, 2571, 593, 260, 110, 2959, 1196, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 148: [296, 318, 2571, 593, 260, 110, 2858, 1196, 527, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 149: [318, 356, 296, 2571, 260, 2858, 593, 1198, 608, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 150: [318, 356, 296, 2571, 593, 260, 110, 589, 480, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 151: [356, 318, 296, 2571, 593, 260, 480, 110, 589, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 152: [318, 356, 110, 2959, 589, 1196, 4993, 480, 47, 1198]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 153: [318, 593, 32, 1210, 47, 260, 608, 2762, 592, 480]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 154: [318, 296, 2571, 593, 260, 110, 527, 1196, 2959, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 155: [356, 318, 296, 2571, 593, 260, 110, 480, 589, 2858]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 156: [318, 110, 2959, 4993, 1196, 527, 47, 5952, 7153, 589]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 157: [318, 356, 296, 2571, 593, 260, 110, 589, 480, 2858]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 158: [356, 318, 296, 2571, 593, 260, 110, 480, 589, 2858]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 159: [2571, 260, 480, 589, 593, 1198, 1210, 780, 32, 592]\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 160: [318, 593, 1198, 50, 150, 58559, 1136, 1089, 457, 1193]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 161: [356, 296, 2571, 593, 260, 110, 858, 2959, 1196, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 162: [2571, 593, 260, 110, 2959, 589, 4993, 527, 480, 1196]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 163: [318, 356, 296, 2571, 593, 260, 110, 2858, 1196, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 164: [318, 356, 296, 2571, 593, 260, 2959, 4993, 527, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 165: [356, 318, 296, 593, 260, 110, 480, 589, 2858, 1198]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 166: [110, 589, 480, 150, 47, 780, 1270, 377, 457, 364]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 167: [356, 318, 2571, 480, 1196, 2959, 589, 527, 47, 858]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 168: [318, 2571, 260, 593, 110, 1196, 2858, 527, 1198, 150]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 169: [356, 296, 593, 260, 110, 589, 2959, 480, 1196, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 170: [356, 296, 2571, 260, 2858, 2959, 1196, 527, 1198, 4993]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 171: [356, 2571, 593, 260, 110, 589, 2959, 480, 527, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 172: [318, 356, 296, 2571, 260, 110, 2959, 2858, 589, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 173: [356, 318, 296, 2571, 593, 260, 2858, 2959, 1196, 47]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 174: [318, 296, 2571, 593, 260, 110, 589, 2858, 2959, 1196]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 175: [296, 318, 356, 2571, 260, 593, 858, 1196, 110, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 176: [318, 356, 2571, 260, 589, 480, 2959, 4993, 1196, 2858]\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 177: [356, 296, 110, 589, 2959, 5952, 2028, 32, 2762, 780]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 178: [318, 356, 260, 1196, 2959, 150, 858, 50, 7153, 1210]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 179: [2571, 260, 2959, 2858, 527, 1196, 4993, 1198, 5952, 7153]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 180: [296, 318, 356, 593, 260, 110, 527, 2858, 858, 47]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 181: [318, 2571, 260, 2858, 1198, 2959, 527, 589, 1210, 480]\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 182: [318, 296, 480, 2762, 608, 364, 780, 588, 58559, 377]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 183: [318, 356, 2571, 296, 593, 260, 110, 480, 2858, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 184: [356, 318, 296, 2571, 593, 260, 110, 2959, 1196, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 185: [356, 318, 296, 2571, 593, 260, 110, 480, 2959, 589]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 186: [356, 318, 296, 110, 2959, 2858, 527, 150, 47, 5952]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 187: [296, 318, 593, 260, 2959, 858, 32, 150, 2762, 2028]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 188: [318, 296, 2571, 593, 260, 110, 2959, 4993, 589, 480]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 189: [356, 318, 296, 260, 110, 480, 2858, 1196, 589, 150]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 190: [318, 356, 296, 593, 260, 110, 2858, 589, 1196, 480]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 191: [318, 356, 2571, 260, 593, 1196, 527, 2858, 2959, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 192: [356, 318, 2571, 260, 589, 780, 2959, 2858, 1198, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 193: [2571, 110, 32, 47, 50, 608, 1210, 4993, 2858, 58559]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 194: [318, 296, 356, 2571, 593, 260, 110, 2858, 1196, 2959]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 195: [318, 296, 356, 593, 260, 110, 2959, 1198, 480, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 196: [296, 356, 318, 260, 593, 2858, 1198, 1196, 527, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 197: [593, 260, 110, 480, 2858, 780, 1198, 150, 1196, 2959]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 198: [527, 4993, 7153, 5952, 858, 150, 588, 457, 4306, 590]\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "User 199: [110, 2959, 480, 4993, 7153, 5952, 32, 1270, 2028, 364]\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step\n",
      "User 200: [318, 2571, 593, 110, 589, 480, 527, 858, 50, 2028]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step\n",
      "User 201: [318, 2571, 110, 2959, 527, 4993, 858, 5952, 7153, 47]\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 202: [356, 527, 480, 4993, 7153, 47, 5952, 50, 2028, 457]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 203: [356, 593, 260, 2959, 1196, 527, 2858, 480, 589, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 204: [318, 356, 2571, 593, 260, 110, 589, 1196, 4993, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 205: [318, 296, 2571, 593, 260, 110, 589, 480, 2858, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 206: [318, 356, 296, 2571, 593, 260, 110, 2959, 527, 1196]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 207: [356, 318, 296, 2571, 593, 260, 110, 2959, 1196, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 208: [356, 318, 2571, 296, 593, 260, 480, 2858, 1198, 780]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 209: [296, 260, 110, 1196, 527, 858, 150, 47, 4993, 50]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 210: [318, 356, 593, 110, 480, 2959, 2858, 527, 150, 47]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 211: [356, 593, 110, 480, 589, 1196, 527, 4993, 150, 50]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 212: [593, 110, 480, 1196, 2959, 527, 1198, 150, 47, 858]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 213: [318, 356, 296, 2571, 593, 260, 110, 480, 2858, 2959]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 214: [296, 318, 356, 593, 2571, 260, 2959, 527, 1196, 858]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 215: [2571, 260, 110, 480, 589, 1196, 47, 150, 858, 50]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 216: [296, 318, 356, 2571, 260, 593, 110, 1196, 527, 50]\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 217: [318, 593, 2858, 2959, 527, 480, 32, 1196, 150, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 218: [356, 318, 296, 2571, 593, 260, 858, 47, 2959, 110]\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 219: [356, 296, 110, 1196, 527, 50, 858, 32, 1210, 590]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 220: [318, 593, 110, 2858, 527, 4993, 457, 3578, 592, 4995]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 221: [260, 110, 1196, 4993, 150, 50, 47, 7153, 5952, 2028]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 222: [356, 318, 593, 260, 480, 2858, 589, 1196, 4993, 7153]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 223: [356, 593, 260, 480, 589, 4993, 1196, 2959, 2858, 5952]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 224: [318, 356, 2571, 296, 110, 589, 480, 2858, 2959, 1196]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 225: [318, 356, 296, 2571, 593, 260, 110, 589, 480, 1198]\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 226: [318, 296, 593, 110, 527, 858, 150, 50, 2028, 2762]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 227: [356, 2959, 480, 2858, 527, 1198, 150, 4993, 5952, 50]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 228: [356, 296, 593, 110, 589, 2858, 480, 2959, 1198, 527]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "User 229: [2571, 593, 260, 2959, 1196, 2858, 4993, 5952, 7153, 858]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 230: [356, 2571, 296, 480, 260, 1210, 2858, 32, 1198, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 231: [318, 356, 260, 110, 480, 780, 2858, 1198, 2959, 150]\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 232: [593, 110, 2858, 589, 50, 32, 592, 608, 457, 590]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 233: [593, 260, 1196, 1198, 480, 32, 589, 1210, 858, 457]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 234: [318, 356, 296, 593, 260, 110, 2959, 589, 527, 2858]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 235: [356, 2571, 260, 2959, 2858, 1196, 4993, 5952, 1198, 7153]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 236: [356, 318, 296, 2571, 593, 260, 110, 2959, 4993, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 237: [296, 593, 2571, 260, 2959, 527, 480, 1196, 589, 110]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 238: [356, 318, 296, 2571, 260, 110, 2959, 4993, 589, 2858]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 239: [318, 356, 2959, 527, 150, 5952, 50, 2762, 1210, 377]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 240: [318, 356, 2571, 593, 260, 2858, 1198, 2959, 1196, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 241: [318, 356, 260, 110, 589, 2959, 4993, 480, 1196, 5952]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 242: [356, 2571, 110, 260, 1196, 527, 2959, 2858, 858, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 243: [318, 2571, 296, 593, 260, 110, 589, 480, 2858, 780]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 244: [296, 318, 260, 527, 2959, 2858, 4993, 858, 480, 1198]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 245: [356, 318, 2571, 296, 593, 260, 32, 480, 2959, 1210]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 246: [318, 593, 110, 589, 527, 480, 150, 47, 2858, 858]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 247: [296, 2571, 1196, 5952, 858, 608, 457, 364, 592, 1580]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 248: [356, 318, 296, 593, 110, 2858, 2959, 47, 150, 480]\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 249: [318, 1196, 858, 377, 592, 608, 590, 1036, 4995, 1197]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 250: [356, 296, 2571, 260, 593, 110, 2858, 589, 1196, 527]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 251: [260, 110, 1196, 150, 2959, 2858, 858, 1198, 4993, 50]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "User 252: [318, 356, 296, 2571, 593, 260, 110, 2959, 589, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 253: [296, 318, 356, 2571, 260, 593, 110, 1196, 2858, 527]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 254: [318, 296, 527, 589, 1198, 858, 608, 377, 4306, 3578]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 255: [356, 318, 296, 32, 593, 2571, 260, 608, 47, 2959]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 256: [356, 318, 296, 593, 110, 480, 589, 2858, 2959, 780]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 257: [318, 356, 2571, 296, 260, 593, 110, 589, 480, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "User 258: [318, 356, 296, 593, 260, 858, 47, 1196, 110, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 259: [356, 318, 593, 47, 2959, 296, 858, 1210, 608, 2571]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 260: [318, 356, 2571, 593, 260, 110, 4993, 1196, 2858, 589]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 261: [318, 260, 593, 110, 1196, 527, 1198, 150, 2959, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 262: [356, 2571, 32, 2858, 1210, 527, 47, 1196, 150, 858]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 263: [356, 296, 2571, 260, 589, 480, 2959, 1198, 7153, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 264: [318, 296, 593, 260, 480, 589, 2959, 110, 4993, 47]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 265: [356, 318, 296, 589, 2959, 4993, 5952, 47, 7153, 527]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 266: [318, 593, 858, 527, 47, 4993, 50, 7153, 608, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 267: [318, 356, 2571, 296, 480, 2959, 4993, 150, 527, 7153]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 268: [296, 356, 318, 2571, 593, 858, 110, 2959, 32, 50]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 269: [318, 356, 296, 2571, 593, 260, 110, 2959, 527, 589]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 270: [356, 318, 296, 593, 2571, 260, 2959, 110, 480, 589]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 271: [356, 318, 296, 2571, 593, 260, 110, 480, 589, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 272: [356, 2571, 296, 593, 260, 110, 480, 1198, 2858, 780]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 273: [356, 318, 2571, 593, 260, 110, 2959, 527, 1196, 4993]\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 274: [318, 593, 110, 2858, 4993, 7153, 150, 1210, 588, 4226]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 275: [110, 1196, 589, 150, 7153, 2028, 47, 457, 608, 588]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 276: [318, 296, 2571, 593, 110, 589, 2959, 527, 480, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 277: [318, 356, 2571, 296, 593, 110, 589, 480, 2858, 1198]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 278: [356, 296, 2571, 593, 260, 110, 1196, 150, 2858, 858]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "User 279: [527, 480, 589, 5952, 47, 150, 32, 2762, 457, 592]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 280: [318, 296, 2571, 593, 260, 110, 589, 2959, 480, 4993]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 281: [296, 356, 318, 2571, 593, 260, 110, 2959, 1196, 47]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 282: [356, 593, 110, 480, 1196, 4993, 150, 1198, 7153, 5952]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 283: [296, 2571, 260, 1196, 858, 527, 2959, 50, 47, 1198]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step\n",
      "User 284: [2571, 593, 260, 110, 2858, 2959, 1196, 780, 527, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 285: [356, 318, 2571, 593, 260, 2959, 110, 4993, 47, 589]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 286: [318, 356, 296, 260, 1196, 858, 2858, 589, 1198, 480]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 287: [296, 356, 318, 260, 593, 2571, 32, 858, 47, 1210]\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 288: [2858, 1196, 527, 47, 858, 50, 457, 1270, 4306, 590]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 289: [318, 356, 296, 2571, 593, 260, 110, 2858, 2959, 1196]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 290: [356, 296, 2571, 260, 110, 527, 2959, 1196, 858, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 291: [296, 356, 318, 2571, 260, 593, 110, 2858, 150, 527]\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 292: [318, 2858, 150, 527, 50, 457, 47, 608, 4226, 1036]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 293: [356, 318, 296, 2571, 260, 2959, 480, 1196, 4993, 589]\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 294: [318, 2571, 593, 2858, 110, 47, 480, 2959, 527, 32]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 295: [356, 296, 2571, 593, 260, 858, 47, 1196, 608, 110]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 296: [593, 260, 1196, 4993, 858, 150, 47, 7153, 5952, 589]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 297: [296, 318, 356, 2571, 260, 2858, 110, 1198, 1196, 527]\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 298: [593, 527, 1210, 150, 480, 2762, 457, 2028, 364, 4306]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 299: [356, 318, 296, 2571, 593, 260, 110, 2959, 4993, 589]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 300: [296, 2571, 260, 110, 1196, 1198, 150, 50, 858, 47]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 301: [318, 2571, 593, 260, 2959, 1196, 1198, 47, 858, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 302: [318, 356, 2571, 593, 2959, 527, 1196, 2858, 4993, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 303: [356, 318, 2571, 296, 593, 260, 110, 480, 2858, 780]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 304: [110, 1196, 2959, 4993, 50, 7153, 5952, 47, 858, 592]\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 305: [110, 589, 858, 47, 150, 480, 780, 588, 364, 4306]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 306: [356, 318, 2571, 296, 593, 260, 589, 480, 110, 780]\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 307: [356, 527, 858, 2028, 150, 457, 590, 380, 377, 1704]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 308: [296, 32, 1210, 593, 608, 47, 480, 592, 780, 2858]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 309: [356, 260, 593, 2858, 110, 480, 589, 1196, 150, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 310: [356, 318, 296, 2571, 593, 260, 2959, 110, 480, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 311: [356, 318, 2571, 296, 32, 593, 1210, 480, 47, 608]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 312: [318, 296, 593, 2858, 2959, 1198, 4993, 150, 858, 50]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 313: [356, 110, 47, 1196, 527, 5952, 7153, 2762, 150, 364]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 314: [2571, 2858, 1198, 2959, 527, 1210, 150, 1196, 1270, 2762]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 315: [318, 356, 296, 593, 2571, 260, 2959, 527, 32, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 316: [296, 318, 356, 593, 260, 110, 1196, 527, 2959, 1198]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 317: [356, 318, 480, 589, 1196, 4993, 527, 1198, 5952, 7153]\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 318: [356, 296, 110, 589, 2959, 527, 1198, 150, 5952, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 319: [296, 593, 260, 589, 1196, 480, 2858, 4993, 150, 47]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 320: [356, 318, 296, 2571, 593, 260, 480, 589, 2959, 110]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 321: [318, 2571, 296, 260, 110, 2858, 1198, 1196, 2959, 47]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step\n",
      "User 322: [318, 356, 260, 593, 1198, 527, 32, 1210, 589, 1196]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 323: [356, 296, 593, 2959, 260, 2858, 858, 47, 2762, 4993]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "User 324: [296, 356, 318, 593, 260, 2571, 32, 47, 608, 2959]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 325: [2571, 260, 110, 2858, 527, 2959, 858, 1198, 47, 50]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 326: [356, 260, 480, 2959, 589, 1196, 150, 858, 1198, 2762]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 327: [318, 2571, 296, 593, 260, 110, 589, 480, 780, 2858]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 328: [593, 2571, 260, 2858, 527, 110, 32, 4993, 457, 3578]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 329: [356, 32, 1210, 296, 47, 608, 2959, 2571, 293, 1193]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 330: [260, 527, 5952, 50, 32, 150, 457, 6539, 4995, 1036]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973us/step\n",
      "User 331: [356, 2571, 260, 110, 1196, 527, 2858, 1198, 4993, 47]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 332: [2571, 260, 2959, 2858, 527, 1198, 858, 2762, 4306, 457]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 333: [296, 356, 318, 2571, 593, 260, 858, 2959, 110, 1196]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 334: [356, 318, 593, 110, 2858, 1198, 480, 589, 527, 50]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 335: [593, 260, 2959, 4993, 480, 47, 5952, 858, 7153, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 336: [318, 356, 2571, 260, 110, 1196, 4993, 480, 1198, 5952]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 337: [356, 296, 2571, 593, 589, 2959, 480, 2858, 1196, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 338: [356, 593, 2571, 260, 1196, 4993, 47, 32, 110, 608]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 339: [2571, 260, 110, 589, 1196, 4993, 780, 858, 5952, 7153]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 340: [2571, 593, 260, 589, 2858, 1198, 1196, 2959, 4993, 377]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 341: [318, 296, 2571, 593, 260, 110, 589, 2959, 527, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 342: [318, 356, 296, 2571, 593, 260, 110, 2858, 1196, 589]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 343: [318, 356, 110, 589, 480, 47, 50, 2028, 1210, 32]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 344: [356, 296, 593, 110, 589, 480, 2858, 2959, 1198, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "User 345: [318, 356, 296, 2571, 593, 260, 110, 589, 2959, 527]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 346: [356, 318, 2571, 593, 260, 110, 589, 480, 1196, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 347: [318, 296, 2571, 593, 260, 2858, 2959, 1198, 1196, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 348: [356, 2571, 110, 589, 527, 150, 480, 47, 780, 7153]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 349: [318, 2571, 260, 110, 2858, 2959, 1196, 4993, 527, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step\n",
      "User 350: [318, 356, 296, 2571, 593, 2959, 2858, 527, 1198, 480]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 351: [318, 356, 260, 110, 2858, 589, 1196, 527, 1198, 150]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "User 352: [110, 589, 480, 2959, 150, 858, 1210, 2762, 588, 780]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "User 353: [356, 296, 2571, 260, 2858, 1196, 527, 2959, 1198, 589]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "User 354: [110, 589, 1198, 4993, 5952, 7153, 364, 608, 3578, 590]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step\n",
      "User 355: [318, 356, 296, 2571, 593, 260, 110, 589, 2959, 2858]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "User 356: [296, 593, 260, 110, 589, 2959, 480, 527, 1196, 150]\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "User 357: [356, 296, 2571, 593, 260, 480, 4993, 150, 47, 5952]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step\n",
      "User 358: [296, 318, 356, 2571, 260, 593, 110, 1196, 527, 2858]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 359: [318, 356, 260, 110, 1196, 527, 2959, 4993, 858, 7153]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 360: [296, 318, 356, 2571, 260, 593, 110, 2858, 1196, 527]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 361: [2571, 296, 480, 110, 1198, 2858, 592, 2959, 32, 1210]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 362: [318, 356, 296, 593, 527, 480, 150, 2858, 1198, 50]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 363: [356, 593, 110, 2959, 527, 47, 1196, 858, 2762, 4993]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 364: [356, 318, 296, 2571, 593, 110, 480, 589, 2959, 150]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step\n",
      "User 365: [356, 2571, 593, 296, 480, 32, 1210, 589, 2959, 592]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 366: [318, 356, 296, 2571, 593, 260, 2858, 527, 1196, 4993]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "User 367: [296, 2571, 593, 110, 589, 480, 2858, 2959, 4993, 780]\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 368: [318, 593, 260, 2959, 527, 4993, 7153, 5952, 32, 150]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "User 369: [356, 318, 296, 593, 110, 2858, 1196, 1198, 480, 527]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 370: [356, 110, 480, 1196, 2959, 4993, 527, 150, 2028, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step\n",
      "User 371: [318, 356, 296, 593, 260, 110, 589, 2959, 527, 1196]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 372: [318, 2571, 2858, 2959, 480, 4993, 7153, 1210, 5952, 32]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "User 373: [2571, 260, 589, 1196, 2858, 4993, 2959, 527, 7153, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 374: [296, 2571, 593, 260, 2959, 1196, 4993, 527, 2858, 5952]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 375: [296, 2571, 593, 260, 110, 1196, 527, 2858, 150, 858]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 376: [318, 356, 296, 593, 260, 110, 2959, 2858, 1196, 1198]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 377: [318, 356, 296, 2571, 593, 260, 110, 2858, 1196, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 378: [296, 356, 2571, 260, 110, 1196, 589, 480, 1198, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 379: [296, 2571, 593, 260, 2858, 1196, 1198, 2959, 4993, 50]\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 380: [527, 2858, 4993, 1198, 858, 150, 2028, 377, 1270, 608]\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step\n",
      "User 381: [296, 2571, 593, 110, 2959, 2858, 527, 1198, 858, 2028]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 382: [260, 110, 2858, 527, 589, 47, 480, 32, 1270, 608]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 383: [356, 318, 296, 2571, 260, 593, 110, 2858, 1198, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 384: [356, 318, 2571, 593, 260, 2858, 110, 1198, 1196, 150]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 385: [356, 2571, 260, 480, 110, 2858, 2959, 4993, 5952, 7153]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 386: [356, 2571, 593, 260, 1196, 2858, 2959, 527, 47, 858]\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 387: [318, 593, 527, 480, 150, 7153, 5952, 1210, 2028, 2762]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step\n",
      "User 388: [296, 356, 318, 2571, 593, 260, 858, 2959, 47, 110]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 389: [318, 356, 2571, 296, 593, 110, 589, 480, 2959, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 390: [356, 318, 296, 2571, 593, 260, 589, 4993, 480, 110]\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step\n",
      "User 391: [296, 593, 260, 1198, 7153, 5952, 32, 364, 592, 590]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 392: [296, 356, 318, 2571, 260, 593, 110, 858, 1196, 150]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 393: [356, 318, 296, 593, 260, 110, 4993, 527, 1196, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 394: [296, 318, 593, 2571, 260, 1196, 527, 858, 2959, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 395: [318, 2571, 260, 593, 2858, 1198, 150, 1210, 32, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 396: [296, 356, 593, 2571, 260, 2959, 858, 47, 1196, 4993]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 397: [356, 318, 296, 593, 2571, 260, 110, 2959, 527, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 398: [318, 356, 296, 2571, 593, 260, 110, 2959, 4993, 589]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "User 399: [318, 2959, 858, 47, 608, 50, 32, 2028, 364, 1210]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 400: [110, 589, 2959, 527, 480, 2858, 858, 150, 5952, 780]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "User 401: [318, 296, 2571, 593, 260, 110, 2858, 480, 589, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "User 402: [318, 356, 2571, 260, 110, 480, 2959, 2858, 1196, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step\n",
      "User 403: [318, 356, 2571, 260, 593, 1196, 527, 2858, 2959, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "User 404: [2571, 260, 2858, 1196, 2959, 1198, 589, 4993, 50, 780]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 405: [356, 318, 593, 260, 110, 480, 589, 2858, 527, 1196]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 406: [296, 356, 318, 2571, 260, 593, 110, 1196, 858, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 407: [318, 356, 296, 593, 260, 110, 589, 480, 2858, 2959]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 408: [356, 296, 2571, 593, 260, 110, 589, 2959, 480, 2858]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 409: [356, 2571, 260, 110, 1196, 527, 1198, 150, 2959, 50]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 410: [356, 318, 2571, 260, 110, 589, 2959, 480, 4993, 1196]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step\n",
      "User 411: [296, 2571, 260, 2959, 2858, 1196, 4993, 1198, 5952, 7153]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 412: [318, 356, 296, 260, 110, 589, 2959, 1196, 527, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "User 413: [356, 593, 2571, 260, 110, 47, 480, 589, 5952, 1196]\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 414: [2571, 2959, 527, 480, 589, 7153, 50, 592, 4306, 377]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 415: [2571, 110, 589, 527, 1196, 4993, 7153, 5952, 47, 2028]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step\n",
      "User 416: [296, 593, 32, 858, 1210, 47, 50, 58559, 364, 1193]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 417: [356, 593, 260, 110, 589, 480, 1196, 780, 527, 150]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 418: [356, 2571, 260, 480, 589, 110, 1196, 858, 47, 1198]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 419: [318, 356, 260, 1196, 589, 2959, 527, 1198, 150, 4993]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 420: [318, 593, 260, 110, 2959, 1196, 589, 1198, 150, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 421: [356, 296, 2571, 260, 110, 2959, 1196, 150, 4993, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 422: [296, 356, 2571, 593, 110, 2858, 1196, 527, 1198, 50]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 423: [2571, 593, 260, 527, 2858, 858, 2959, 1198, 50, 47]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 424: [356, 260, 480, 110, 589, 2858, 1196, 4993, 1198, 5952]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 425: [780, 50, 32, 1270, 590, 380, 4226, 58559, 1, 1265]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 426: [2571, 296, 593, 260, 589, 110, 2858, 780, 1198, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 427: [356, 318, 296, 2571, 593, 260, 2959, 2858, 480, 47]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 428: [296, 318, 593, 260, 2959, 527, 47, 858, 32, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 429: [318, 356, 2571, 296, 593, 260, 110, 589, 480, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "User 430: [296, 356, 593, 260, 110, 1196, 527, 2959, 2858, 4993]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step\n",
      "User 431: [356, 296, 318, 593, 2571, 260, 47, 858, 2959, 32]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 432: [356, 593, 480, 150, 1198, 5952, 377, 2028, 50, 588]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "User 433: [356, 2571, 260, 480, 589, 4993, 47, 2858, 5952, 1198]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 434: [356, 110, 2858, 1198, 5952, 50, 480, 457, 592, 364]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step\n",
      "User 435: [356, 296, 2571, 593, 260, 110, 589, 2959, 527, 1196]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 436: [296, 2571, 593, 260, 110, 2858, 2959, 1196, 4993, 1198]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 437: [2571, 593, 589, 2858, 1196, 2959, 480, 4993, 527, 7153]\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step\n",
      "User 438: [318, 296, 593, 2858, 1196, 1198, 47, 858, 2028, 50]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "User 439: [318, 296, 593, 260, 110, 2959, 150, 1196, 858, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 440: [318, 356, 296, 2571, 593, 260, 110, 589, 480, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "User 441: [318, 356, 593, 260, 110, 480, 780, 2858, 1198, 150]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "User 442: [356, 318, 296, 32, 2571, 593, 1210, 608, 47, 480]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 443: [318, 593, 589, 480, 780, 1196, 1198, 5952, 527, 377]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "User 444: [356, 296, 2571, 260, 110, 2959, 1196, 2858, 4993, 858]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 445: [318, 1196, 2858, 2959, 589, 1198, 480, 50, 858, 2028]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 446: [318, 2571, 260, 110, 2858, 1196, 2959, 4993, 1198, 5952]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 447: [296, 2571, 260, 2959, 527, 1196, 4993, 2858, 780, 1198]\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986us/step\n",
      "User 448: [318, 110, 480, 527, 150, 858, 2028, 457, 4306, 364]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 449: [296, 356, 318, 260, 2571, 593, 1196, 47, 527, 110]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "User 450: [318, 356, 296, 2571, 593, 260, 2959, 480, 527, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "User 451: [296, 318, 356, 2571, 260, 110, 1196, 2858, 527, 2959]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 452: [318, 296, 593, 260, 2959, 527, 4993, 150, 2858, 5952]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 453: [318, 2959, 1198, 858, 5952, 7153, 150, 50, 1270, 588]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 454: [318, 356, 2571, 593, 260, 110, 2959, 480, 589, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 455: [296, 2571, 260, 2959, 527, 1196, 2858, 4993, 1198, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 456: [356, 318, 296, 2571, 593, 110, 2959, 4993, 150, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 457: [296, 318, 356, 2571, 593, 1196, 527, 858, 47, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 458: [318, 2571, 593, 260, 2959, 480, 4993, 1196, 527, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 459: [318, 356, 296, 593, 260, 110, 2959, 1196, 527, 858]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 460: [356, 296, 593, 260, 110, 2959, 4993, 1196, 527, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 461: [296, 593, 47, 2959, 858, 32, 608, 1210, 364, 293]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step\n",
      "User 462: [356, 296, 110, 480, 150, 4306, 457, 4226, 588, 58559]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 463: [318, 2571, 593, 260, 589, 480, 2959, 2858, 4993, 1196]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 464: [318, 356, 260, 527, 2959, 4993, 1198, 150, 50, 2028]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "User 465: [318, 296, 260, 110, 2959, 527, 589, 2858, 4993, 480]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 466: [296, 2571, 593, 260, 110, 480, 589, 2858, 150, 1198]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step\n",
      "User 467: [318, 356, 296, 260, 2571, 32, 1210, 2858, 608, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 468: [356, 296, 2571, 593, 260, 480, 589, 2858, 1198, 1196]\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 469: [356, 296, 2959, 527, 4993, 1198, 7153, 5952, 50, 150]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 470: [2571, 260, 593, 2959, 2858, 1198, 1196, 150, 1210, 780]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 471: [318, 593, 260, 110, 1196, 2858, 858, 1198, 47, 50]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 472: [356, 296, 2571, 593, 260, 110, 527, 2959, 1196, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step \n",
      "User 473: [296, 356, 2571, 260, 593, 110, 2858, 1196, 527, 1198]\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 474: [2571, 260, 480, 1198, 7153, 5952, 457, 58559, 1265, 79132]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 475: [356, 318, 2571, 296, 593, 110, 589, 480, 2959, 780]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 476: [2571, 593, 260, 110, 2959, 527, 2858, 1196, 4993, 7153]\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 477: [110, 2959, 4993, 858, 7153, 50, 1210, 588, 1270, 3578]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 478: [356, 318, 296, 2571, 260, 593, 110, 2858, 1198, 589]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 479: [356, 318, 296, 2571, 2858, 2959, 150, 47, 1196, 527]\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 480: [593, 1196, 480, 5952, 2028, 58559, 648, 79132, 8961, 1240]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 481: [296, 356, 318, 2571, 593, 260, 110, 2959, 1196, 4993]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 482: [356, 318, 2571, 296, 593, 260, 480, 589, 110, 2858]\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step\n",
      "User 483: [593, 1196, 480, 5952, 1198, 588, 457, 608, 3578, 592]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "User 484: [296, 2571, 593, 260, 110, 589, 1196, 150, 1198, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "User 485: [318, 356, 296, 2571, 260, 589, 2959, 4993, 480, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 486: [356, 2571, 593, 260, 1196, 2858, 527, 2959, 1198, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 487: [296, 356, 260, 110, 1196, 47, 32, 527, 480, 50]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 488: [356, 296, 2571, 593, 260, 110, 589, 2858, 1196, 2959]\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 489: [318, 296, 2571, 260, 1196, 480, 589, 527, 50, 32]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 490: [318, 593, 260, 110, 2858, 1196, 1198, 4993, 527, 50]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 491: [318, 296, 2571, 593, 260, 110, 2959, 527, 589, 1196]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 492: [356, 318, 2571, 296, 593, 110, 589, 480, 780, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 493: [356, 318, 296, 593, 2959, 47, 4993, 480, 5952, 150]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 494: [296, 318, 356, 260, 593, 110, 2858, 527, 150, 50]\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 495: [356, 296, 2571, 593, 260, 110, 2959, 1196, 589, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 496: [356, 296, 318, 2571, 593, 260, 2959, 110, 47, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 497: [356, 296, 527, 858, 47, 32, 1196, 110, 1210, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 498: [296, 2571, 593, 260, 589, 2959, 527, 1196, 2858, 4993]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 499: [356, 318, 296, 2571, 593, 260, 110, 2959, 4993, 1196]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 500: [356, 318, 296, 2571, 593, 480, 589, 2959, 47, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 501: [356, 318, 2571, 296, 593, 260, 480, 589, 110, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 502: [296, 356, 2571, 260, 593, 858, 110, 1196, 150, 47]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 503: [2571, 260, 593, 110, 1196, 1198, 589, 4993, 32, 47]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "User 504: [318, 356, 2571, 593, 260, 110, 480, 589, 2959, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 505: [318, 356, 296, 2571, 260, 110, 589, 2858, 2959, 1196]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 506: [318, 296, 356, 2571, 260, 593, 110, 2858, 1196, 589]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 507: [318, 2571, 296, 260, 589, 2858, 1198, 2959, 32, 780]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 508: [356, 318, 2571, 260, 480, 589, 296, 32, 593, 1210]\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 509: [356, 318, 593, 110, 2959, 2858, 527, 47, 480, 858]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 510: [296, 356, 318, 2571, 260, 2858, 32, 1198, 2959, 110]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 511: [296, 593, 260, 110, 589, 480, 1196, 2858, 4993, 150]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 512: [356, 296, 2571, 260, 589, 2858, 1196, 2959, 4993, 1198]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 513: [356, 318, 2571, 1196, 527, 1198, 2959, 47, 50, 858]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 514: [318, 110, 2858, 527, 50, 32, 858, 3578, 608, 592]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "User 515: [296, 2571, 593, 110, 1196, 589, 858, 4993, 150, 2858]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 516: [356, 318, 296, 2571, 593, 260, 110, 2959, 4993, 1196]\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 517: [2571, 296, 589, 2959, 32, 47, 1198, 4993, 7153, 608]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "User 518: [356, 318, 296, 2571, 593, 260, 110, 2959, 1196, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 519: [318, 296, 2571, 593, 260, 110, 589, 2959, 527, 1196]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 520: [296, 593, 260, 110, 480, 2959, 2858, 1196, 150, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 521: [356, 318, 296, 2571, 593, 260, 110, 589, 2959, 480]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 522: [296, 593, 589, 480, 150, 527, 47, 592, 32, 377]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 523: [356, 296, 2571, 593, 260, 110, 589, 1196, 480, 150]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 524: [356, 296, 2571, 2858, 2959, 4993, 150, 1198, 5952, 7153]\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 525: [318, 296, 110, 1196, 527, 4993, 589, 1198, 5952, 7153]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step\n",
      "User 526: [318, 356, 2571, 260, 110, 1196, 527, 1198, 50, 150]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 527: [318, 356, 296, 2571, 110, 589, 2959, 2858, 480, 4993]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 528: [356, 318, 260, 858, 2959, 1196, 47, 1198, 50, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 529: [318, 356, 296, 2571, 593, 110, 589, 2858, 1196, 480]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 530: [318, 296, 2571, 260, 110, 589, 2858, 1196, 2959, 1198]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 531: [356, 318, 296, 2571, 260, 110, 2959, 1196, 858, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 532: [296, 356, 2571, 260, 1196, 4993, 150, 50, 7153, 5952]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 533: [318, 296, 593, 2571, 260, 47, 858, 4993, 5952, 110]\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 534: [318, 296, 110, 589, 2959, 4993, 150, 527, 7153, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 535: [296, 318, 356, 260, 593, 50, 1196, 110, 1198, 32]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 536: [296, 318, 2571, 260, 1196, 858, 50, 1198, 47, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 537: [356, 296, 593, 260, 2959, 47, 858, 480, 589, 110]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 538: [318, 356, 2571, 296, 593, 260, 110, 589, 480, 780]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 539: [356, 2571, 296, 593, 260, 110, 589, 480, 2858, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 540: [318, 356, 593, 110, 1196, 589, 2858, 527, 1198, 150]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 541: [296, 2571, 260, 110, 2858, 1196, 1198, 527, 2959, 150]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 542: [318, 593, 260, 110, 589, 480, 2858, 1198, 780, 1196]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 543: [318, 296, 593, 260, 110, 589, 480, 2858, 780, 1196]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 544: [356, 318, 296, 2571, 593, 260, 110, 2959, 4993, 589]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 545: [318, 356, 296, 2571, 593, 260, 110, 589, 480, 2858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 546: [318, 356, 2571, 296, 593, 260, 110, 780, 1198, 2959]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 547: [296, 356, 318, 2571, 260, 593, 858, 50, 47, 150]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 548: [318, 296, 2571, 593, 110, 2959, 589, 4993, 527, 480]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 549: [356, 296, 593, 47, 260, 858, 110, 364, 5952, 608]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 550: [296, 593, 260, 110, 2858, 589, 2959, 1196, 527, 480]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step\n",
      "User 551: [2571, 296, 480, 2858, 1198, 1196, 150, 780, 47, 592]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 552: [318, 593, 260, 110, 2858, 480, 589, 2959, 4993, 1198]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "User 553: [318, 356, 2571, 593, 260, 110, 589, 1196, 527, 4993]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 554: [318, 356, 296, 260, 110, 589, 480, 2959, 4993, 1196]\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 555: [593, 527, 589, 4993, 858, 5952, 7153, 47, 2028, 588]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 556: [356, 318, 296, 2571, 593, 260, 110, 2959, 589, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 557: [356, 296, 593, 110, 480, 589, 2858, 1198, 2959, 47]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 558: [318, 356, 2571, 593, 260, 110, 2959, 589, 480, 527]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 559: [356, 2571, 593, 2959, 2858, 1196, 4993, 527, 5952, 7153]\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 560: [356, 318, 1196, 527, 480, 858, 150, 2762, 2028, 780]\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 561: [110, 4993, 1198, 47, 858, 1210, 150, 4306, 1270, 588]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 562: [356, 593, 260, 480, 2959, 2858, 780, 1198, 4993, 5952]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 563: [318, 296, 2571, 593, 260, 480, 110, 589, 2959, 2858]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 564: [296, 2571, 593, 260, 110, 480, 589, 2858, 1196, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 565: [318, 2571, 593, 260, 110, 2858, 1196, 2959, 1198, 4993]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step\n",
      "User 566: [318, 296, 2571, 593, 260, 589, 2858, 1196, 1198, 2959]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 567: [318, 296, 2571, 593, 32, 2959, 47, 527, 4993, 2858]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 568: [318, 2571, 260, 110, 1196, 527, 2959, 858, 150, 2858]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 569: [318, 2571, 593, 260, 110, 589, 480, 2858, 1196, 1198]\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 570: [296, 593, 480, 1196, 4993, 5952, 50, 32, 2762, 588]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 571: [356, 318, 296, 593, 2571, 260, 110, 480, 589, 2959]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 572: [318, 356, 296, 2571, 593, 110, 2858, 2959, 47, 50]\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 573: [110, 2858, 527, 858, 50, 2762, 457, 608, 32, 4306]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 574: [356, 2571, 296, 260, 589, 480, 2959, 1196, 2858, 4993]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 575: [318, 356, 2571, 593, 260, 110, 589, 2959, 480, 2858]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 576: [318, 296, 356, 2571, 593, 260, 110, 589, 480, 1196]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 577: [296, 2571, 2858, 2959, 527, 4993, 1198, 589, 480, 7153]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 578: [318, 356, 296, 2571, 593, 260, 110, 589, 2959, 527]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step\n",
      "User 579: [318, 296, 593, 260, 110, 589, 2959, 480, 4993, 150]\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 580: [318, 589, 1198, 150, 50, 2028, 1210, 377, 4306, 380]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 581: [296, 593, 260, 110, 589, 480, 1196, 2858, 4993, 858]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "User 582: [356, 318, 296, 2571, 260, 110, 480, 589, 2858, 527]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 583: [318, 2571, 260, 2959, 2858, 527, 1198, 110, 480, 589]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 584: [318, 356, 2571, 260, 2959, 4993, 527, 1196, 2858, 7153]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 585: [318, 356, 296, 2571, 593, 260, 110, 589, 480, 1196]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 586: [356, 296, 593, 589, 2959, 480, 2858, 1196, 150, 780]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 587: [318, 296, 593, 260, 110, 589, 480, 1196, 1198, 2959]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 588: [318, 2571, 260, 110, 1196, 2858, 2959, 4993, 1198, 7153]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 589: [2571, 260, 110, 589, 2959, 480, 1196, 4993, 2858, 858]\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 590: [2571, 593, 2959, 527, 589, 480, 1210, 608, 3578, 590]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 591: [356, 318, 296, 593, 110, 480, 589, 2858, 780, 1198]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 592: [2571, 260, 2959, 527, 1196, 2858, 4993, 858, 1198, 7153]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 593: [296, 32, 1198, 2959, 47, 457, 589, 480, 58559, 2028]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 594: [318, 356, 296, 260, 110, 2858, 1198, 1196, 2959, 4993]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 595: [356, 318, 296, 2571, 593, 260, 110, 2959, 1196, 858]\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 596: [318, 296, 593, 110, 2858, 480, 1198, 5952, 47, 2028]\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 597: [318, 356, 296, 2571, 593, 1196, 527, 2959, 150, 4993]\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 598: [296, 318, 356, 260, 2571, 593, 110, 2858, 527, 50]\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 599: [110, 527, 50, 1210, 589, 3578, 590, 1193, 6377, 1265]\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step\n",
      "User 600: [318, 480, 50, 150, 2028, 457, 364, 3578, 58559, 4226]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step\n",
      "User 601: [356, 296, 593, 260, 110, 589, 527, 480, 1196, 2858]\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 602: [2571, 2858, 2959, 527, 1198, 1196, 1210, 858, 2762, 4993]\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 603: [318, 260, 47, 4993, 150, 5952, 7153, 480, 2028, 589]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 604: [318, 356, 2571, 260, 589, 480, 2858, 2959, 1196, 1198]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 605: [318, 2571, 593, 2858, 589, 1196, 2959, 1198, 527, 50]\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 606: [296, 593, 1198, 47, 150, 608, 588, 364, 457, 4306]\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 607: [356, 318, 2571, 260, 589, 2959, 4993, 780, 5952, 7153]\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step\n",
      "User 608: [2858, 4993, 1198, 589, 2028, 1210, 377, 58559, 4995, 1265]\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "User 609: [356, 296, 2571, 593, 260, 2959, 2858, 1198, 527, 1196]\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "User 610: [150, 5952, 1210, 457, 364, 588, 780, 590, 4995, 1580]\n"
     ]
    }
   ],
   "source": [
    "def enhanced_recommend_movies_v3(model, user_to_idx, movie_to_idx, train_data, n=10, \n",
    "                                 user_rating_threshold=None, min_ratings=10, popularity_weight=0.4, confidence_weight=0.3):\n",
    "    \"\"\"\n",
    "    Further enhanced recommendation function:\n",
    "    - User-specific rating thresholds\n",
    "    - Personalized weighting based on user behavior\n",
    "    - More sophisticated confidence scoring\n",
    "    \"\"\"\n",
    "    # Calculate movie statistics (count, mean, std)\n",
    "    movie_stats = train_data.groupby('movieId').agg({\n",
    "        'rating': ['count', 'mean', 'std']\n",
    "    }).droplevel(0, axis=1)\n",
    "    \n",
    "    # Filter movies with enough ratings\n",
    "    valid_movies = set(movie_stats[movie_stats['count'] >= min_ratings].index)\n",
    "    \n",
    "    # Determine user-specific rating thresholds if not provided\n",
    "    if user_rating_threshold is None:\n",
    "        user_rating_threshold = {}\n",
    "        for user_id in train_data['userId'].unique():\n",
    "            user_ratings = train_data[train_data['userId'] == user_id]\n",
    "            user_rating_threshold[user_id] = user_ratings['rating'].median()\n",
    "    \n",
    "    # Calculate user preferences (movies rated >= user-specific rating threshold)\n",
    "    user_preferences = {}\n",
    "    for user_id in train_data['userId'].unique():\n",
    "        user_ratings = train_data[train_data['userId'] == user_id]\n",
    "        liked_movies = user_ratings[user_ratings['rating'] >= user_rating_threshold[user_id]]['movieId']\n",
    "        \n",
    "        if len(liked_movies) > 0:\n",
    "            user_preferences[user_id] = liked_movies.tolist()\n",
    "    \n",
    "    unwatched = unwatched_movies(train_data)\n",
    "    recommendations = {}\n",
    "    \n",
    "    for user_id, movies in unwatched.items():\n",
    "        if user_id not in user_preferences:\n",
    "            continue  # Skip if there are no preferences\n",
    "        \n",
    "        user_unwatched = set(movies).intersection(valid_movies)\n",
    "        if not user_unwatched:\n",
    "            continue\n",
    "        \n",
    "        user_idx = user_to_idx[user_id]\n",
    "        movie_indices = [movie_to_idx[movie_id] for movie_id in user_unwatched]\n",
    "        \n",
    "        user_array = np.array([user_idx] * len(movie_indices))\n",
    "        movie_array = np.array(movie_indices)\n",
    "        \n",
    "        # Get predictions for unwatched movies\n",
    "        predictions = model.predict([user_array, movie_array]).flatten()\n",
    "        \n",
    "        # Calculate personalized scores with weighted components\n",
    "        predictions_with_scores = []\n",
    "        for movie_id, pred_rating in zip(user_unwatched, predictions):\n",
    "            movie_idx = movie_to_idx[movie_id]\n",
    "            \n",
    "            # Confidence scoring: Takes into account the user behavior and movie statistics\n",
    "            confidence = 1 - (1 / (1 + movie_stats.loc[movie_id, 'count'] / 100))\n",
    "            popularity = movie_stats.loc[movie_id, 'count'] / movie_stats['count'].max()\n",
    "            \n",
    "            # Weighted factors: Personal behavior, prediction accuracy, popularity\n",
    "            adjusted_score = (confidence_weight * confidence + \n",
    "                              popularity_weight * popularity + \n",
    "                              (1 - popularity_weight - confidence_weight) * pred_rating)\n",
    "            \n",
    "            predictions_with_scores.append((movie_id, adjusted_score))\n",
    "        \n",
    "        # Sort by adjusted score and get top N recommendations\n",
    "        top_n_movie_ids = [\n",
    "            movie_id for movie_id, _ in sorted(predictions_with_scores, key=lambda x: x[1], reverse=True)[:n]\n",
    "        ]\n",
    "        \n",
    "        recommendations[user_id] = top_n_movie_ids\n",
    "        print(f\"User {user_id}: {top_n_movie_ids}\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Example usage:\n",
    "top_10_recommendations_v3 = enhanced_recommend_movies_v3(model, user_id_mapping, movie_id_mapping, train_data, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision \t16.2951%\n",
      "Recall \t\t7.1915%\n",
      "F-Measure \t9.9790%\n",
      "NDCG \t\t0.1890\n"
     ]
    }
   ],
   "source": [
    "def dcg(recommended_movies, actual_movies):\n",
    "    dcg_value = 0.0\n",
    "    for i, movie in enumerate(recommended_movies):\n",
    "        if movie in actual_movies:\n",
    "            dcg_value += 1 / np.log2(i + 2)  \n",
    "    return dcg_value\n",
    "\n",
    "def idcg(actual_movies, length):\n",
    "    idcg_value = 0.0\n",
    "    for i in range(min(len(actual_movies), length)):\n",
    "        idcg_value += 1 / np.log2(i + 2)\n",
    "    return idcg_value\n",
    "\n",
    "def recommendation_performance(recommendations, test_data):\n",
    "    running_precision, running_recall, running_ndcg = 0, 0, 0\n",
    "\n",
    "    for user_id, recommended_movies in recommendations.items():\n",
    "        actual_movies = test_data[test_data['userId'] == user_id]['movieId']\n",
    "\n",
    "        intersection = len(set(recommended_movies) & set(actual_movies))\n",
    "        precision = (intersection / len(recommended_movies)) * 100\n",
    "        recall = (intersection / len(actual_movies)) * 100\n",
    "\n",
    "        running_precision += precision\n",
    "        running_recall += recall\n",
    "\n",
    "        dcg_value = dcg(list(recommended_movies), list(actual_movies))\n",
    "        idcg_value = idcg(list(actual_movies), len(recommended_movies))\n",
    "        ndcg = dcg_value / idcg_value if idcg_value > 0 else 0\n",
    "        running_ndcg += ndcg\n",
    "\n",
    "    precision = running_precision / len(recommendations)\n",
    "    recall = running_recall / len(recommendations)\n",
    "    f_measure = 2 * (precision * recall) / (precision + recall)\n",
    "    ndcg = running_ndcg / len(recommendations)\n",
    "\n",
    "    return precision, recall, f_measure, ndcg\n",
    "\n",
    "print(\"Precision \\t%.4f%%\\nRecall \\t\\t%.4f%%\\nF-Measure \\t%.4f%%\\nNDCG \\t\\t%.4f\" % recommendation_performance(top_10_recommendations_v3, test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_sci_439",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
